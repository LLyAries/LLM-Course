"""
çŸ¥è¯†åº“é€‚åº”å¾®è°ƒ
å¾®è°ƒæ£€ç´¢æ¨¡å‹ä»¥é€‚åº”ç‰¹å®šæ°”å€™é¢†åŸŸ
"""

import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import pickle
import json
from pathlib import Path
from typing import Dict, List, Tuple, Optional
import warnings
import logging
from datetime import datetime

warnings.filterwarnings('ignore')

from sentence_transformers import SentenceTransformer, losses, InputExample
from sentence_transformers.evaluation import InformationRetrievalEvaluator
from sentence_transformers.datasets import SentencesDataset

from config import config


class KnowledgeDataset(Dataset):
    """çŸ¥è¯†åº“æ£€ç´¢æ•°æ®é›†"""

    def __init__(self, queries: List[str], documents: List[str],
                 labels: List[List[int]], query_embeddings: np.ndarray = None,
                 doc_embeddings: np.ndarray = None):
        """
        åˆå§‹åŒ–æ•°æ®é›†

        Args:
            queries: æŸ¥è¯¢æ–‡æœ¬åˆ—è¡¨
            documents: æ–‡æ¡£æ–‡æœ¬åˆ—è¡¨
            labels: æ¯ä¸ªæŸ¥è¯¢çš„ç›¸å…³æ–‡æ¡£ç´¢å¼•åˆ—è¡¨
            query_embeddings: é¢„è®¡ç®—çš„æŸ¥è¯¢åµŒå…¥
            doc_embeddings: é¢„è®¡ç®—çš„æ–‡æ¡£åµŒå…¥
        """
        self.queries = queries
        self.documents = documents
        self.labels = labels
        self.query_embeddings = query_embeddings
        self.doc_embeddings = doc_embeddings

        # éªŒè¯æ•°æ®
        assert len(queries) == len(labels), "æŸ¥è¯¢å’Œæ ‡ç­¾æ•°é‡ä¸ä¸€è‡´"

        print(f"çŸ¥è¯†åº“æ•°æ®é›†: {len(queries)} æŸ¥è¯¢, {len(documents)} æ–‡æ¡£")
        print(f"å¹³å‡ç›¸å…³æ–‡æ¡£æ•°: {np.mean([len(l) for l in labels]):.2f}")

    def __len__(self):
        return len(self.queries)

    def __getitem__(self, idx):
        query = self.queries[idx]
        relevant_docs = self.labels[idx]

        # éšæœºé€‰æ‹©ä¸€ä¸ªç›¸å…³æ–‡æ¡£
        if relevant_docs:
            pos_idx = np.random.choice(relevant_docs)
            pos_doc = self.documents[pos_idx]
        else:
            # å¦‚æœæ²¡æœ‰ç›¸å…³æ–‡æ¡£ï¼Œéšæœºé€‰æ‹©ä¸€ä¸ª
            pos_idx = np.random.randint(0, len(self.documents))
            pos_doc = self.documents[pos_idx]

        # éšæœºé€‰æ‹©ä¸€ä¸ªä¸ç›¸å…³æ–‡æ¡£
        neg_candidates = [i for i in range(len(self.documents))
                          if i not in relevant_docs]
        neg_idx = np.random.choice(neg_candidates) if neg_candidates else np.random.randint(0, len(self.documents))
        neg_doc = self.documents[neg_idx]

        return {
            'query': query,
            'positive': pos_doc,
            'negative': neg_doc,
            'query_idx': idx,
            'pos_idx': pos_idx,
            'neg_idx': neg_idx
        }

    def create_triplets(self) -> List[Tuple[str, str, str]]:
        """åˆ›å»ºä¸‰å…ƒç»„æ•°æ®"""
        triplets = []

        for idx in range(len(self.queries)):
            query = self.queries[idx]
            relevant_docs = self.labels[idx]

            if not relevant_docs:
                continue

            # å¯¹æ¯ä¸ªç›¸å…³æ–‡æ¡£ï¼Œåˆ›å»ºä¸€ä¸ªè´Ÿæ ·æœ¬
            for pos_idx in relevant_docs:
                pos_doc = self.documents[pos_idx]

                # å¯»æ‰¾è´Ÿæ ·æœ¬
                neg_candidates = [i for i in range(len(self.documents))
                                  if i not in relevant_docs]
                if neg_candidates:
                    neg_idx = np.random.choice(neg_candidates)
                    neg_doc = self.documents[neg_idx]

                    triplets.append((query, pos_doc, neg_doc))

        return triplets

    def create_triplet_examples(self) -> List[InputExample]:
        """åˆ›å»ºä¸‰å…ƒç»„InputExampleåˆ—è¡¨ï¼Œç”¨äºsentence-transformersè®­ç»ƒ"""
        examples = []
        triplets = self.create_triplets()

        for query, pos_doc, neg_doc in triplets:
            # åˆ›å»ºInputExampleï¼ŒtextsåŒ…å«ä¸‰ä¸ªå…ƒç´ ï¼š[anchor, positive, negative]
            examples.append(InputExample(texts=[query, pos_doc, neg_doc]))

        return examples


class KnowledgeAdaptor:
    """çŸ¥è¯†åº“é€‚åº”å™¨"""

    def __init__(self):
        self.config = config
        self.device = self.config.get_device()

        # æ¨¡å‹
        self.model = None
        self.train_loss = None
        self.evaluator = None

        # è®­ç»ƒå†å²
        self.history = {
            'train_loss': [],
            'val_ndcg': [],
            'val_map': [],
            'val_recall': []
        }

        # è¾“å‡ºç›®å½•
        self.output_dir = Path(self.config.paths["finetune_output"]) / "knowledge_adaptor"
        self.output_dir.mkdir(parents=True, exist_ok=True)

        # æ—¥å¿—
        self._setup_logging()

        print(self.config)

    def _setup_logging(self):
        """è®¾ç½®æ—¥å¿—"""
        import logging

        log_file = self.output_dir / "training.log"

        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_file),
                logging.StreamHandler()
            ]
        )

        self.logger = logging.getLogger(__name__)

    def load_knowledge_base(self) -> Dict:
        """åŠ è½½çŸ¥è¯†åº“"""
        print("åŠ è½½çŸ¥è¯†åº“...")

        knowledge_path = Path(self.config.paths["knowledge_json"])

        if not knowledge_path.exists():
            raise FileNotFoundError(f"çŸ¥è¯†åº“æ–‡ä»¶ä¸å­˜åœ¨: {knowledge_path}")

        with open(knowledge_path, 'r', encoding='utf-8') as f:
            knowledge_base = json.load(f)

        # åŠ è½½åµŒå…¥å‘é‡
        embeddings_path = Path(self.config.paths["knowledge_embeddings"])
        if embeddings_path.exists():
            embeddings = np.load(embeddings_path)
        else:
            embeddings = None

        print(f"çŸ¥è¯†åº“æ¡ç›®: {len(knowledge_base.get('items', []))}")

        return {
            'knowledge_base': knowledge_base,
            'embeddings': embeddings
        }

    def create_synthetic_queries(self, knowledge_base: Dict, num_queries: int = 1000) -> Dict:
        """åˆ›å»ºåˆæˆæŸ¥è¯¢ï¼ˆç”¨äºæ¼”ç¤ºï¼Œå®é™…åº”ç”¨åº”ä½¿ç”¨çœŸå®æŸ¥è¯¢ï¼‰"""
        items = knowledge_base.get('items', [])

        queries = []
        documents = []
        query_to_doc_labels = {}  # æŸ¥è¯¢ç´¢å¼• -> ç›¸å…³æ–‡æ¡£ç´¢å¼•åˆ—è¡¨

        # æ–‡æ¡£æ–‡æœ¬
        for i, item in enumerate(items):
            # åˆ›å»ºæ–‡æ¡£æ–‡æœ¬
            doc_text = f"{item.get('category', '')} {item.get('title', '')} {item.get('scientific_basis', '')}"
            if 'warning_indicators' in item:
                doc_text += f" {item['warning_indicators']}"
            documents.append(doc_text)

        # ç”ŸæˆæŸ¥è¯¢ï¼ˆåŸºäºæ–‡æ¡£å†…å®¹ï¼‰
        for query_idx in range(num_queries):
            # éšæœºé€‰æ‹©ä¸€ä¸ªæ–‡æ¡£ä½œä¸ºæŸ¥è¯¢åŸºç¡€
            doc_idx = np.random.randint(0, len(items))
            item = items[doc_idx]

            # åˆ›å»ºæŸ¥è¯¢ï¼ˆæ¨¡æ‹Ÿç”¨æˆ·é—®é¢˜ï¼‰
            if 'temperature' in item.get('scientific_basis', '').lower():
                query = f"æ¸©åº¦{np.random.randint(30, 40)}â„ƒ æ¹¿åº¦{np.random.randint(30, 80)}% å¤©æ°”æƒ…å†µ"
            elif 'rain' in item.get('scientific_basis', '').lower():
                query = f"é™é›¨{np.random.randint(10, 100)}mm é¢„æµ‹"
            elif 'wind' in item.get('scientific_basis', '').lower():
                query = f"é£é€Ÿ{np.random.randint(5, 20)}m/s å½±å“"
            else:
                query = f"{item.get('category', '')}ç›¸å…³å’¨è¯¢"

            queries.append(query)

            # ç›¸å…³æ–‡æ¡£ï¼ˆé€‰æ‹©åŒç±»åˆ«çš„æ–‡æ¡£ï¼‰
            category = item.get('category', '')
            relevant_docs = []

            for j, other_item in enumerate(items):
                if other_item.get('category', '') == category:
                    relevant_docs.append(j)

            # ç¡®ä¿è‡³å°‘æœ‰ä¸€ä¸ªç›¸å…³æ–‡æ¡£
            if not relevant_docs:
                relevant_docs = [doc_idx]

            query_to_doc_labels[query_idx] = relevant_docs

        return {
            'queries': queries,
            'documents': documents,
            'labels': [query_to_doc_labels[i] for i in range(len(queries))]
        }

    def create_datasets(self, split_ratio: Tuple = (0.7, 0.15, 0.15)) -> Dict[str, KnowledgeDataset]:
        """åˆ›å»ºæ•°æ®é›†"""
        # åŠ è½½çŸ¥è¯†åº“
        knowledge_data = self.load_knowledge_base()
        knowledge_base = knowledge_data['knowledge_base']

        # åˆ›å»ºåˆæˆæŸ¥è¯¢æ•°æ®
        synthetic_data = self.create_synthetic_queries(knowledge_base, num_queries=2000)

        n_queries = len(synthetic_data['queries'])
        indices = np.random.permutation(n_queries)

        train_size = int(n_queries * split_ratio[0])
        val_size = int(n_queries * split_ratio[1])

        train_idx = indices[:train_size]
        val_idx = indices[train_size:train_size + val_size]
        test_idx = indices[train_size + val_size:]

        datasets = {}

        for name, idx in [('train', train_idx), ('val', val_idx), ('test', test_idx)]:
            queries = [synthetic_data['queries'][i] for i in idx]
            labels = [synthetic_data['labels'][i] for i in idx]

            datasets[name] = KnowledgeDataset(
                queries=queries,
                documents=synthetic_data['documents'],
                labels=labels
            )

        print(f"æ•°æ®é›†åˆ’åˆ†: è®­ç»ƒé›†={len(datasets['train'])}, "
              f"éªŒè¯é›†={len(datasets['val'])}, æµ‹è¯•é›†={len(datasets['test'])}")

        return datasets

    def build_model(self, model_name: str = None):
        """æ„å»ºæ¨¡å‹"""
        if model_name is None:
            model_name = self.config.knowledge_config.get('base_model',
                                                          'paraphrase-multilingual-MiniLM-L12-v2')

        print(f"æ„å»ºæ£€ç´¢æ¨¡å‹: {model_name}")

        # åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
        self.model = SentenceTransformer(model_name)

        print(f"æ¨¡å‹ç»´åº¦: {self.model.get_sentence_embedding_dimension()}")

        return self.model

    def create_evaluator(self, val_dataset: KnowledgeDataset):
        """åˆ›å»ºè¯„ä¼°å™¨"""
        # å‡†å¤‡è¯„ä¼°æ•°æ®
        # å°†æŸ¥è¯¢åˆ—è¡¨è½¬æ¢ä¸ºå­—å…¸ {query_id: query_text}
        queries = {str(i): query for i, query in enumerate(val_dataset.queries)}
        # å°†æ–‡æ¡£åˆ—è¡¨è½¬æ¢ä¸ºå­—å…¸ {doc_id: doc_text}
        corpus = {str(i): doc for i, doc in enumerate(val_dataset.documents)}

        # æŸ¥è¯¢ -> ç›¸å…³æ–‡æ¡£æ˜ å°„
        query_to_relevant_docs = {}
        for i, relevant_docs in enumerate(val_dataset.labels):
            query_to_relevant_docs[str(i)] = {str(doc_idx) for doc_idx in relevant_docs}

        # åˆ›å»ºè¯„ä¼°å™¨
        self.evaluator = InformationRetrievalEvaluator(
            queries=queries,
            corpus=corpus,
            relevant_docs=query_to_relevant_docs,
            show_progress_bar=True,
            batch_size=32,
            name="climate_retrieval"
        )

    def train(self, train_dataset: KnowledgeDataset, val_dataset: KnowledgeDataset):
        """è®­ç»ƒæ¨¡å‹"""
        print("å¼€å§‹è®­ç»ƒæ£€ç´¢æ¨¡å‹...")

        # åˆ›å»ºè®­ç»ƒç¤ºä¾‹ï¼ˆä½¿ç”¨sentence-transformersçš„InputExampleæ ¼å¼ï¼‰
        train_examples = train_dataset.create_triplet_examples()
        print(f"è®­ç»ƒä¸‰å…ƒç»„æ•°é‡: {len(train_examples)}")

        if len(train_examples) == 0:
            raise ValueError("æ²¡æœ‰è®­ç»ƒæ ·æœ¬ï¼Œè¯·æ£€æŸ¥æ•°æ®")

        # åˆ›å»ºsentence-transformersçš„æ•°æ®é›†
        train_data = SentencesDataset(train_examples, model=self.model)
        train_dataloader = DataLoader(train_data, shuffle=True, batch_size=self.config.training_config["batch_size"])

        # åˆ›å»ºæŸå¤±å‡½æ•°
        train_loss = losses.TripletLoss(model=self.model)

        # åˆ›å»ºè¯„ä¼°å™¨
        self.create_evaluator(val_dataset)

        # è®­ç»ƒé…ç½®
        epochs = self.config.training_config["epochs"]
        warmup_steps = int(len(train_dataloader) * 0.1)

        # è®­ç»ƒæ¨¡å‹ - æ³¨æ„ï¼šç§»é™¤äº†batch_sizeå‚æ•°
        self.model.fit(
            train_objectives=[(train_dataloader, train_loss)],
            evaluator=self.evaluator,
            epochs=epochs,
            warmup_steps=warmup_steps,
            optimizer_params={'lr': self.config.training_config["learning_rate"]},
            output_path=str(self.output_dir / "model"),
            save_best_model=True,
            show_progress_bar=True,
            evaluation_steps=100,
            checkpoint_path=str(self.output_dir / "checkpoints"),
            checkpoint_save_steps=500
        )

        # åŠ è½½æœ€ä½³æ¨¡å‹
        self.model = SentenceTransformer(str(self.output_dir / "model"))

        # è®°å½•å†å²
        self._record_training_history()

        self.logger.info("æ£€ç´¢æ¨¡å‹è®­ç»ƒå®Œæˆ")

        return self.history

    def _record_training_history(self):
        """è®°å½•è®­ç»ƒå†å²"""
        # è¿™é‡Œéœ€è¦æ ¹æ®å®é™…è®­ç»ƒè¿‡ç¨‹è®°å½•
        # ç”±äºsentence-transformersçš„è®­ç»ƒå†å²è®°å½•æ–¹å¼ä¸åŒï¼Œè¿™é‡Œç®€åŒ–å¤„ç†
        pass

    def test(self, test_dataset: KnowledgeDataset) -> Dict:
        """æµ‹è¯•æ¨¡å‹"""
        print("æµ‹è¯•æ£€ç´¢æ¨¡å‹...")

        # ç¼–ç æ‰€æœ‰æ–‡æ¡£
        corpus_embeddings = self.model.encode(
            test_dataset.documents,
            show_progress_bar=True,
            convert_to_numpy=True,
            normalize_embeddings=True
        )

        # æµ‹è¯•æŒ‡æ ‡
        results = {
            'precision_at_1': [],
            'precision_at_3': [],
            'precision_at_5': [],
            'recall_at_5': [],
            'ndcg_at_5': [],
            'mrr': []
        }

        # å¯¹æ¯ä¸ªæŸ¥è¯¢è¿›è¡Œè¯„ä¼°
        for query_idx, query in enumerate(test_dataset.queries):
            # ç¼–ç æŸ¥è¯¢
            query_embedding = self.model.encode(
                [query],
                convert_to_numpy=True,
                normalize_embeddings=True
            )[0]

            # è®¡ç®—ç›¸ä¼¼åº¦
            similarities = np.dot(corpus_embeddings, query_embedding)

            # è·å–æ’å
            ranked_indices = np.argsort(similarities)[::-1]

            # ç›¸å…³æ–‡æ¡£
            relevant_docs = set(test_dataset.labels[query_idx])

            # è®¡ç®—æŒ‡æ ‡
            for k in [1, 3, 5]:
                retrieved_at_k = ranked_indices[:k]
                relevant_retrieved = len([idx for idx in retrieved_at_k if idx in relevant_docs])
                precision = relevant_retrieved / k if k > 0 else 0
                results[f'precision_at_{k}'].append(precision)

            # Recall@5
            relevant_retrieved = len([idx for idx in ranked_indices[:5] if idx in relevant_docs])
            recall = relevant_retrieved / len(relevant_docs) if relevant_docs else 0
            results['recall_at_5'].append(recall)

            # NDCG@5
            dcg = 0
            for rank, idx in enumerate(ranked_indices[:5], 1):
                if idx in relevant_docs:
                    dcg += 1 / np.log2(rank + 1)

            # ç†æƒ³DCG
            ideal_ranking = min(5, len(relevant_docs))
            idcg = sum(1 / np.log2(i + 1) for i in range(1, ideal_ranking + 1))
            ndcg = dcg / idcg if idcg > 0 else 0
            results['ndcg_at_5'].append(ndcg)

            # MRR
            for rank, idx in enumerate(ranked_indices, 1):
                if idx in relevant_docs:
                    results['mrr'].append(1.0 / rank)
                    break
            else:
                results['mrr'].append(0.0)

        # è®¡ç®—å¹³å‡æŒ‡æ ‡
        avg_results = {}
        for key, values in results.items():
            avg_results[key] = np.mean(values) if values else 0

        # ä¿å­˜ç»“æœ
        test_path = self.output_dir / "test_results.json"
        with open(test_path, 'w') as f:
            json.dump(avg_results, f, indent=2)

        print("æµ‹è¯•ç»“æœ:")
        for key, value in avg_results.items():
            print(f"  {key}: {value:.4f}")

        return avg_results

    def save_model(self):
        """ä¿å­˜æ¨¡å‹"""
        model_path = self.output_dir / "adapted_model"
        self.model.save(str(model_path))

        # ä¿å­˜é…ç½®
        config_path = model_path / "config.json"
        config_dict = {
            'model_name': self.model.model_name if hasattr(self.model, 'model_name') else str(self.model),
            'embedding_dim': self.model.get_sentence_embedding_dimension(),
            'training_config': self.config.training_config,
            'knowledge_config': self.config.knowledge_config
        }

        with open(config_path, 'w') as f:
            json.dump(config_dict, f, indent=2)

        self.logger.info(f"æ¨¡å‹ä¿å­˜åˆ°: {model_path}")

    def run_pipeline(self):
        """è¿è¡Œå®Œæ•´å¾®è°ƒæµæ°´çº¿"""
        self.logger.info("=" * 60)
        self.logger.info("ğŸ“š çŸ¥è¯†æ£€ç´¢å¾®è°ƒæµæ°´çº¿")
        self.logger.info("=" * 60)

        try:
            # 1. å‡†å¤‡æ•°æ®
            self.logger.info("æ­¥éª¤1: å‡†å¤‡æ•°æ®...")
            datasets = self.create_datasets()

            if 'train' not in datasets or 'val' not in datasets:
                raise ValueError("ç¼ºå°‘è®­ç»ƒé›†æˆ–éªŒè¯é›†")

            # 2. æ„å»ºæ¨¡å‹
            self.logger.info("æ­¥éª¤2: æ„å»ºæ¨¡å‹...")
            self.build_model()

            # 3. è®­ç»ƒæ¨¡å‹
            self.logger.info("æ­¥éª¤3: è®­ç»ƒæ¨¡å‹...")
            history = self.train(datasets['train'], datasets['val'])

            # 4. æµ‹è¯•æ¨¡å‹
            self.logger.info("æ­¥éª¤4: æµ‹è¯•æ¨¡å‹...")
            test_results = {}
            if 'test' in datasets:
                test_results = self.test(datasets['test'])

            # 5. ä¿å­˜æ¨¡å‹
            self.logger.info("æ­¥éª¤5: ä¿å­˜æ¨¡å‹...")
            self.save_model()

            self.logger.info("=" * 60)
            self.logger.info("âœ… çŸ¥è¯†æ£€ç´¢å¾®è°ƒå®Œæˆ!")
            self.logger.info("=" * 60)

            return {
                'model': self.model,
                'history': history,
                'test_results': test_results
            }

        except Exception as e:
            self.logger.error(f"å¾®è°ƒå¤±è´¥: {e}")
            import traceback
            self.logger.error(traceback.format_exc())
            raise


def main():
    """ä¸»å‡½æ•°"""
    # åˆ›å»ºé€‚åº”å™¨
    adaptor = KnowledgeAdaptor()

    # è¿è¡Œå¾®è°ƒæµæ°´çº¿
    results = adaptor.run_pipeline()

    return results


if __name__ == "__main__":
    main()