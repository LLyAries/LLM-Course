"""
åœŸå£¤ç±»å‹åˆ†ç±»å¾®è°ƒ - ä¿®å¤ç‰ˆæœ¬
è‡ªåŠ¨æ£€æµ‹å®é™…åœŸå£¤ç±»å‹æ•°é‡
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import numpy as np
import pickle
import json
from pathlib import Path
from typing import Dict, List, Tuple, Optional
import warnings
warnings.filterwarnings('ignore')

from config import config
from eurosat_loader import EuroSATFeatureExtractor, Config as EuroSATConfig

class SoilDataset(Dataset):
    """åœŸå£¤ç±»å‹æ•°æ®é›†"""

    def __init__(self, features: np.ndarray, labels: np.ndarray,
                 soil_types: List[str], transform=None, augment: bool = False):
        """
        åˆå§‹åŒ–æ•°æ®é›†

        Args:
            features: ç‰¹å¾æ•°ç»„ [n_samples, feature_dim]
            labels: æ ‡ç­¾æ•°ç»„ [n_samples]
            soil_types: å®é™…åœŸå£¤ç±»å‹åˆ—è¡¨
            transform: æ•°æ®è½¬æ¢
            augment: æ˜¯å¦æ•°æ®å¢å¼º
        """
        self.features = features.astype(np.float32)
        self.labels = labels.astype(np.int64)
        self.soil_types = soil_types
        self.num_classes = len(soil_types)
        self.transform = transform
        self.augment = augment

        # éªŒè¯æ ‡ç­¾èŒƒå›´
        unique_labels = np.unique(self.labels)
        max_label = self.labels.max()

        if max_label >= self.num_classes:
            print(f"è­¦å‘Š: æ ‡ç­¾å€¼ {max_label} è¶…å‡ºç±»åˆ«èŒƒå›´ {self.num_classes-1}")
            # é‡æ–°æ˜ å°„æ ‡ç­¾
            label_mapping = {old: new for new, old in enumerate(unique_labels)}
            self.labels = np.array([label_mapping[l] for l in self.labels])

        # ç»Ÿè®¡
        self.class_counts = np.bincount(self.labels, minlength=self.num_classes)

        print(f"æ•°æ®é›†: {len(self.features)} æ ·æœ¬, {self.num_classes} ç±»åˆ«")
        print(f"ç±»åˆ«åˆ†å¸ƒ: {dict(zip(self.soil_types, self.class_counts))}")

    def __len__(self):
        return len(self.features)

    def __getitem__(self, idx):
        feature = self.features[idx]
        label = self.labels[idx]

        # æ•°æ®å¢å¼º
        if self.augment and self.transform:
            feature = self.transform(feature)

        # è½¬æ¢ä¸ºå¼ é‡
        feature = torch.from_numpy(feature)

        return {
            'features': feature,
            'labels': label,
            'idx': idx
        }

    def get_class_weights(self):
        """è·å–ç±»åˆ«æƒé‡ï¼ˆç”¨äºå¤„ç†ä¸å¹³è¡¡ï¼‰"""
        total = len(self.labels)
        class_counts = self.class_counts
        # é¿å…é™¤ä»¥é›¶
        class_counts = np.where(class_counts == 0, 1, class_counts)
        weights = total / (self.num_classes * class_counts)
        return torch.FloatTensor(weights)


class SoilClassifier(nn.Module):
    """åœŸå£¤ç±»å‹åˆ†ç±»å™¨"""

    def __init__(self, feature_dim: int = 512, num_classes: int = None):
        super(SoilClassifier, self).__init__()

        self.feature_dim = feature_dim
        self.num_classes = num_classes

        # ç‰¹å¾æŠ•å½±å±‚
        self.projection = nn.Sequential(
            nn.Linear(feature_dim, 256),
            nn.BatchNorm1d(256),
            nn.ReLU(inplace=True),
            nn.Dropout(0.3),

            nn.Linear(256, 128),
            nn.BatchNorm1d(128),
            nn.ReLU(inplace=True),
            nn.Dropout(0.2)
        )

        # åˆ†ç±»å¤´
        if num_classes is not None:
            self.classifier = nn.Sequential(
                nn.Linear(128, 64),
                nn.ReLU(inplace=True),
                nn.Dropout(0.1),
                nn.Linear(64, num_classes)
            )
        else:
            # å»¶è¿Ÿåˆ›å»ºåˆ†ç±»å¤´
            self.classifier = None

        # æ³¨æ„åŠ›æœºåˆ¶
        self.attention = nn.Sequential(
            nn.Linear(128, 64),
            nn.Tanh(),
            nn.Linear(64, 1)
        )

    def build_classifier(self, num_classes: int):
        """åŠ¨æ€æ„å»ºåˆ†ç±»å¤´"""
        self.num_classes = num_classes
        self.classifier = nn.Sequential(
            nn.Linear(128, 64),
            nn.ReLU(inplace=True),
            nn.Dropout(0.1),
            nn.Linear(64, num_classes)
        ).to(next(self.parameters()).device)

    def forward(self, x: torch.Tensor, return_attention: bool = False):
        """
        å‰å‘ä¼ æ’­

        Args:
            x: è¾“å…¥ç‰¹å¾ [batch_size, feature_dim]
            return_attention: æ˜¯å¦è¿”å›æ³¨æ„åŠ›æƒé‡

        Returns:
            åˆ†ç±»logitsæˆ–(logits, attention_weights)
        """
        if self.classifier is None:
            raise ValueError("è¯·å…ˆè°ƒç”¨ build_classifier è®¾ç½®ç±»åˆ«æ•°")

        # ç‰¹å¾æŠ•å½±
        projected = self.projection(x)

        # æ³¨æ„åŠ›æƒé‡
        attention_weights = torch.softmax(self.attention(projected), dim=0)
        weighted_features = projected * attention_weights

        # åˆ†ç±»
        logits = self.classifier(weighted_features)

        if return_attention:
            return logits, attention_weights.squeeze()
        return logits

    def extract_features(self, x: torch.Tensor) -> torch.Tensor:
        """æå–ç‰¹å¾ï¼ˆç”¨äºè¿ç§»å­¦ä¹ ï¼‰"""
        return self.projection(x)


class SoilDataLoader:
    """åœŸå£¤æ•°æ®åŠ è½½å™¨ - ä¿®å¤ç‰ˆæœ¬"""

    def __init__(self, config_path: str = None):
        self.config = config
        self.device = self.config.get_device()

        # è‡ªåŠ¨æ£€æµ‹åœŸå£¤ç±»å‹
        self.actual_soil_types = None
        self.soil_mapping = {}

    def detect_soil_types(self, eurosat_labels: np.ndarray) -> Tuple[List[str], np.ndarray]:
        """æ£€æµ‹å®é™…å­˜åœ¨çš„åœŸå£¤ç±»å‹å¹¶é‡æ–°æ˜ å°„æ ‡ç­¾"""

        soil_config = self.config.soil_config
        eurosat_to_soil = soil_config["eurosat_to_soil"]

        # æ”¶é›†æ‰€æœ‰å‡ºç°çš„åœŸå£¤ç±»å‹
        unique_soil_indices = set()

        for label in eurosat_labels:
            soil_idx = eurosat_to_soil.get(int(label), -1)
            if soil_idx != -1:  # æœ‰æ•ˆçš„åœŸå£¤ç±»å‹
                unique_soil_indices.add(soil_idx)

        # æ’åºå¹¶åˆ›å»ºæ˜ å°„
        sorted_soil_indices = sorted(unique_soil_indices)

        # åˆ›å»ºæ–°çš„æ˜ å°„ï¼šåŸåœŸå£¤ç´¢å¼• -> æ–°è¿ç»­ç´¢å¼•
        new_mapping = {old: new for new, old in enumerate(sorted_soil_indices)}

        # è·å–å¯¹åº”çš„åœŸå£¤ç±»å‹åç§°
        soil_types = [soil_config["soil_types"][idx] for idx in sorted_soil_indices]

        print(f"æ£€æµ‹åˆ° {len(soil_types)} ç§åœŸå£¤ç±»å‹: {soil_types}")
        print(f"åœŸå£¤ç±»å‹æ˜ å°„: {new_mapping}")

        return soil_types, new_mapping

    def load_eurosat_features(self) -> Dict[str, np.ndarray]:
        """åŠ è½½EuroSATç‰¹å¾å¹¶è‡ªåŠ¨æ£€æµ‹åœŸå£¤ç±»å‹"""
        print("åŠ è½½EuroSATç‰¹å¾...")

        features_dir = Path(self.config.paths["eurosat_features"])

        datasets = {}

        # å…ˆåŠ è½½è®­ç»ƒé›†ä»¥ç¡®å®šåœŸå£¤ç±»å‹
        train_file = features_dir / "train_features.pkl"

        if not train_file.exists():
            raise FileNotFoundError(f"è®­ç»ƒç‰¹å¾æ–‡ä»¶ä¸å­˜åœ¨: {train_file}")

        with open(train_file, 'rb') as f:
            train_data = pickle.load(f)

        # æ£€æµ‹åœŸå£¤ç±»å‹
        self.actual_soil_types, self.soil_mapping = self.detect_soil_types(train_data['labels'])

        for split in ['train', 'val', 'test']:
            feature_file = features_dir / f"{split}_features.pkl"

            if not feature_file.exists():
                print(f"è­¦å‘Š: {split}ç‰¹å¾æ–‡ä»¶ä¸å­˜åœ¨: {feature_file}")
                continue

            with open(feature_file, 'rb') as f:
                data = pickle.load(f)

            features = data['features']
            eurosat_labels = data['labels']

            # å°†EuroSATæ ‡ç­¾è½¬æ¢ä¸ºåœŸå£¤ç±»å‹æ ‡ç­¾å¹¶è¿›è¡Œé‡æ–°æ˜ å°„
            soil_labels = []
            valid_indices = []

            for i, label in enumerate(eurosat_labels):
                original_soil_idx = self.config.soil_config["eurosat_to_soil"].get(int(label), -1)
                if original_soil_idx != -1:  # æœ‰æ•ˆçš„åœŸå£¤ç±»å‹
                    # é‡æ–°æ˜ å°„åˆ°è¿ç»­ç´¢å¼•
                    new_soil_idx = self.soil_mapping.get(original_soil_idx)
                    if new_soil_idx is not None:
                        soil_labels.append(new_soil_idx)
                        valid_indices.append(i)

            # è¿‡æ»¤æ— æ•ˆæ ·æœ¬
            if valid_indices:
                features = features[valid_indices]
                soil_labels = np.array(soil_labels)
            else:
                print(f"è­¦å‘Š: {split}é›†æ²¡æœ‰æœ‰æ•ˆçš„åœŸå£¤ç±»å‹æ ·æœ¬")
                continue

            datasets[split] = {
                'features': features,
                'labels': soil_labels,
                'original_labels': eurosat_labels[valid_indices] if valid_indices else [],
                'soil_types': self.actual_soil_types
            }

            print(f"{split}: {len(features)} æ ·æœ¬, {len(np.unique(soil_labels))} åœŸå£¤ç±»å‹")

        return datasets

    def create_datasets(self) -> Dict[str, SoilDataset]:
        """åˆ›å»ºæ•°æ®é›†"""
        data_dict = self.load_eurosat_features()

        datasets = {}
        for split in ['train', 'val', 'test']:
            if split in data_dict:
                data = data_dict[split]
                datasets[split] = SoilDataset(
                    features=data['features'],
                    labels=data['labels'],
                    soil_types=self.actual_soil_types,
                    augment=(split == 'train')
                )

        return datasets

    def create_dataloaders(self, batch_size: int = None) -> Dict[str, DataLoader]:
        """åˆ›å»ºæ•°æ®åŠ è½½å™¨"""
        if batch_size is None:
            batch_size = self.config.training_config["batch_size"]

        datasets = self.create_datasets()
        dataloaders = {}

        for split in ['train', 'val', 'test']:
            if split in datasets:
                shuffle = (split == 'train')
                dataloaders[split] = DataLoader(
                    datasets[split],
                    batch_size=batch_size,
                    shuffle=shuffle,
                    num_workers=4,
                    pin_memory=True
                )

        return dataloaders


class SoilFinetuner:
    """åœŸå£¤ç±»å‹å¾®è°ƒå™¨ - ä¿®å¤ç‰ˆæœ¬"""

    def __init__(self, config_path: str = None):
        self.config = config
        self.device = self.config.get_device()

        # æ•°æ®åŠ è½½å™¨
        self.data_loader = SoilDataLoader()

        # æ¨¡å‹
        self.model = None
        self.criterion = None
        self.optimizer = None
        self.scheduler = None

        # è®­ç»ƒå†å²
        self.history = {
            'train_loss': [], 'train_acc': [],
            'val_loss': [], 'val_acc': [],
            'lr': []
        }

        # è¾“å‡ºç›®å½•
        self.output_dir = Path(self.config.paths["finetune_output"]) / "soil_classifier"
        self.output_dir.mkdir(parents=True, exist_ok=True)

        # æ—¥å¿—
        self._setup_logging()

        print(self.config)

    def _setup_logging(self):
        """è®¾ç½®æ—¥å¿—"""
        import logging

        log_file = self.output_dir / "training.log"

        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_file),
                logging.StreamHandler()
            ]
        )

        self.logger = logging.getLogger(__name__)

    def build_model(self, pretrained_path: str = None, num_classes: int = None) -> nn.Module:
        """æ„å»ºæ¨¡å‹"""
        print("æ„å»ºåœŸå£¤åˆ†ç±»æ¨¡å‹...")

        # å¦‚æœæœªæŒ‡å®šç±»åˆ«æ•°ï¼Œä½¿ç”¨é…ç½®ä¸­çš„é»˜è®¤å€¼
        if num_classes is None:
            num_classes = self.config.soil_config["num_classes"]

        # åŠ è½½EuroSATé¢„è®­ç»ƒæ¨¡å‹
        eurosat_config = EuroSATConfig()
        base_model = EuroSATFeatureExtractor(eurosat_config)

        if pretrained_path and Path(pretrained_path).exists():
            checkpoint = torch.load(pretrained_path, map_location=self.device)
            if 'model_state_dict' in checkpoint:
                base_model.load_state_dict(checkpoint['model_state_dict'])
            else:
                base_model.load_state_dict(checkpoint)
            print(f"åŠ è½½é¢„è®­ç»ƒæƒé‡: {pretrained_path}")
        else:
            print("è­¦å‘Š: æœªæ‰¾åˆ°é¢„è®­ç»ƒæƒé‡ï¼Œä½¿ç”¨éšæœºåˆå§‹åŒ–")

        # å†»ç»“å·ç§¯å±‚
        for param in base_model.conv_layers.parameters():
            param.requires_grad = False

        # åˆ›å»ºåœŸå£¤åˆ†ç±»å™¨ï¼ˆå…ˆä¸æŒ‡å®šç±»åˆ«æ•°ï¼‰
        self.model = SoilClassifier(feature_dim=512)

        # ç¨ååŠ¨æ€æ„å»ºåˆ†ç±»å¤´
        self.model.build_classifier(num_classes)

        # å°†æ¨¡å‹ç§»åŠ¨åˆ°è®¾å¤‡
        self.model = self.model.to(self.device)

        # æŸå¤±å‡½æ•°ï¼ˆå¸¦ç±»åˆ«æƒé‡ï¼‰
        self.criterion = nn.CrossEntropyLoss()

        # ä¼˜åŒ–å™¨
        self.optimizer = optim.AdamW(
            self.model.parameters(),
            lr=self.config.training_config["learning_rate"],
            weight_decay=self.config.training_config["weight_decay"]
        )

        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        self.scheduler = self._create_scheduler()

        print(f"æ¨¡å‹å‚æ•°é‡: {sum(p.numel() for p in self.model.parameters())}")
        print(f"å¯è®­ç»ƒå‚æ•°é‡: {sum(p.numel() for p in self.model.parameters() if p.requires_grad)}")

        return self.model

    def _create_scheduler(self):
        """åˆ›å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨"""
        scheduler_config = self.config.training_config["scheduler"]

        if scheduler_config["name"] == "cosine":
            return optim.lr_scheduler.CosineAnnealingLR(
                self.optimizer,
                T_max=self.config.training_config["epochs"],
                eta_min=scheduler_config["min_lr"]
            )
        elif scheduler_config["name"] == "plateau":
            return optim.lr_scheduler.ReduceLROnPlateau(
                self.optimizer,
                mode='min',
                patience=5,
                factor=0.5,
                min_lr=scheduler_config["min_lr"]
            )
        else:
            return optim.lr_scheduler.StepLR(
                self.optimizer,
                step_size=20,
                gamma=0.1
            )

    def train_epoch(self, dataloader: DataLoader, epoch: int) -> Tuple[float, float]:
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        total_loss = 0
        correct = 0
        total = 0

        from tqdm import tqdm
        pbar = tqdm(dataloader, desc=f'Epoch {epoch+1} [è®­ç»ƒ]')

        for batch_idx, batch in enumerate(pbar):
            features = batch['features'].to(self.device, non_blocking=True)
            labels = batch['labels'].to(self.device, non_blocking=True)

            # å‰å‘ä¼ æ’­
            self.optimizer.zero_grad(set_to_none=True)
            logits = self.model(features)
            loss = self.criterion(logits, labels)

            # åå‘ä¼ æ’­
            loss.backward()

            # æ¢¯åº¦è£å‰ª
            torch.nn.utils.clip_grad_norm_(
                self.model.parameters(),
                self.config.training_config["gradient_clip"]
            )

            self.optimizer.step()

            # ç»Ÿè®¡
            total_loss += loss.item()
            _, predicted = torch.max(logits, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

            # æ›´æ–°è¿›åº¦æ¡
            pbar.set_postfix({
                'loss': f'{loss.item():.4f}',
                'acc': f'{100. * correct / total:.2f}%'
            })

        avg_loss = total_loss / len(dataloader)
        accuracy = 100.0 * correct / total

        return avg_loss, accuracy

    def validate(self, dataloader: DataLoader, epoch: int) -> Tuple[float, float]:
        """éªŒè¯"""
        self.model.eval()
        total_loss = 0
        correct = 0
        total = 0

        all_preds = []
        all_labels = []

        with torch.no_grad():
            from tqdm import tqdm
            pbar = tqdm(dataloader, desc=f'Epoch {epoch+1} [éªŒè¯]')

            for batch_idx, batch in enumerate(pbar):
                features = batch['features'].to(self.device, non_blocking=True)
                labels = batch['labels'].to(self.device, non_blocking=True)

                # å‰å‘ä¼ æ’­
                logits = self.model(features)
                loss = self.criterion(logits, labels)

                # ç»Ÿè®¡
                total_loss += loss.item()
                _, predicted = torch.max(logits, 1)
                total += labels.size(0)
                correct += (predicted == labels).sum().item()

                # æ”¶é›†é¢„æµ‹ç»“æœ
                all_preds.extend(predicted.cpu().numpy())
                all_labels.extend(labels.cpu().numpy())

                # æ›´æ–°è¿›åº¦æ¡
                pbar.set_postfix({
                    'loss': f'{loss.item():.4f}',
                    'acc': f'{100. * correct / total:.2f}%'
                })

        avg_loss = total_loss / len(dataloader)
        accuracy = 100.0 * correct / total

        return avg_loss, accuracy, all_preds, all_labels

    def test(self, dataloader: DataLoader, dataset: SoilDataset) -> Dict:
        """æµ‹è¯•"""
        self.model.eval()

        all_preds = []
        all_labels = []
        all_features = []

        with torch.no_grad():
            from tqdm import tqdm
            for batch in tqdm(dataloader, desc="æµ‹è¯•"):
                features = batch['features'].to(self.device)
                labels = batch['labels'].numpy()

                # å‰å‘ä¼ æ’­
                logits = self.model(features)
                _, predicted = torch.max(logits, 1)

                # æå–ç‰¹å¾
                features_extracted = self.model.extract_features(features)

                all_preds.extend(predicted.cpu().numpy())
                all_labels.extend(labels)
                all_features.extend(features_extracted.cpu().numpy())

        # è®¡ç®—æŒ‡æ ‡
        from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

        accuracy = accuracy_score(all_labels, all_preds)

        # ä½¿ç”¨æ•°æ®é›†çš„å®é™…åœŸå£¤ç±»å‹åç§°
        soil_types = dataset.soil_types if hasattr(dataset, 'soil_types') else self.config.soil_config["soil_types"]

        # ç¡®ä¿æ ‡ç­¾åœ¨ç±»åˆ«èŒƒå›´å†…
        max_label = max(max(all_labels), max(all_preds)) if all_labels and all_preds else 0
        if max_label >= len(soil_types):
            print(f"è­¦å‘Š: æ ‡ç­¾å€¼ {max_label} è¶…å‡ºç±»åˆ«èŒƒå›´ {len(soil_types)-1}")
            # æˆªæ–­åœŸå£¤ç±»å‹åˆ—è¡¨
            soil_types = soil_types[:max_label+1]

        report = classification_report(
            all_labels, all_preds,
            target_names=soil_types,
            output_dict=True
        )
        cm = confusion_matrix(all_labels, all_preds)

        return {
            'accuracy': accuracy,
            'report': report,
            'confusion_matrix': cm,
            'predictions': all_preds,
            'labels': all_labels,
            'features': np.array(all_features),
            'soil_types': soil_types
        }

    def train(self, dataloaders: Dict[str, DataLoader]):
        """è®­ç»ƒæ¨¡å‹"""
        print("å¼€å§‹è®­ç»ƒåœŸå£¤ç±»å‹åˆ†ç±»æ¨¡å‹...")

        # è·å–å®é™…ç±»åˆ«æ•°
        if 'train' in dataloaders and hasattr(dataloaders['train'].dataset, 'num_classes'):
            actual_num_classes = dataloaders['train'].dataset.num_classes
            print(f"æ£€æµ‹åˆ°å®é™…ç±»åˆ«æ•°: {actual_num_classes}")
        else:
            # ä»æ•°æ®é›†ä¸­æ¨æ–­
            for batch in dataloaders['train']:
                labels = batch['labels']
                actual_num_classes = len(torch.unique(labels))
                break

        # é‡æ–°æ„å»ºæ¨¡å‹ï¼ˆå¦‚æœéœ€è¦ï¼‰
        if self.model is None or self.model.num_classes != actual_num_classes:
            print(f"é‡æ–°æ„å»ºæ¨¡å‹ï¼Œç±»åˆ«æ•°: {actual_num_classes}")
            self.build_model(self.config.paths["eurosat_model"], actual_num_classes)

        best_val_acc = 0
        patience_counter = 0
        best_model_state = None

        epochs = self.config.training_config["epochs"]
        patience = self.config.training_config["patience"]

        for epoch in range(epochs):
            # è®­ç»ƒ
            train_loss, train_acc = self.train_epoch(dataloaders['train'], epoch)

            # éªŒè¯
            val_loss, val_acc, val_preds, val_labels = self.validate(dataloaders['val'], epoch)

            # æ›´æ–°å­¦ä¹ ç‡
            if self.scheduler is not None:
                if isinstance(self.scheduler, optim.lr_scheduler.ReduceLROnPlateau):
                    self.scheduler.step(val_loss)
                else:
                    self.scheduler.step()

            # è®°å½•å†å²
            current_lr = self.optimizer.param_groups[0]['lr']
            self.history['train_loss'].append(train_loss)
            self.history['train_acc'].append(train_acc)
            self.history['val_loss'].append(val_loss)
            self.history['val_acc'].append(val_acc)
            self.history['lr'].append(current_lr)

            self.logger.info(
                f"Epoch {epoch+1}/{epochs}: "
                f"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, "
                f"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%, "
                f"LR: {current_lr:.6f}"
            )

            # æ—©åœå’Œä¿å­˜æœ€ä½³æ¨¡å‹
            if val_acc > best_val_acc:
                best_val_acc = val_acc
                patience_counter = 0
                best_model_state = self.model.state_dict().copy()

                # ä¿å­˜æœ€ä½³æ¨¡å‹
                self.save_checkpoint(epoch, 'best')
                self.logger.info(f"æœ€ä½³æ¨¡å‹ä¿å­˜ï¼ŒéªŒè¯å‡†ç¡®ç‡: {val_acc:.2f}%")
            else:
                patience_counter += 1
                if patience_counter >= patience:
                    self.logger.info(f"æ—©åœè§¦å‘ï¼Œåœ¨epoch {epoch+1}")
                    break

        # åŠ è½½æœ€ä½³æ¨¡å‹
        if best_model_state is not None:
            self.model.load_state_dict(best_model_state)

        # ä¿å­˜æœ€ç»ˆæ¨¡å‹
        self.save_checkpoint(epochs - 1, 'final')

        # ä¿å­˜è®­ç»ƒå†å²
        self.save_history()

        self.logger.info(f"è®­ç»ƒå®Œæˆï¼Œæœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_acc:.2f}%")

        return self.history

    def save_checkpoint(self, epoch: int, name: str):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        checkpoint_path = self.output_dir / f"{name}_checkpoint.pth"

        # è·å–åœŸå£¤ç±»å‹ä¿¡æ¯
        soil_types = []
        if hasattr(self, 'data_loader') and hasattr(self.data_loader, 'actual_soil_types'):
            soil_types = self.data_loader.actual_soil_types

        torch.save({
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict() if self.scheduler else None,
            'history': self.history,
            'config': self.config,
            'soil_types': soil_types,
            'num_classes': self.model.num_classes if self.model else None
        }, checkpoint_path)

        self.logger.info(f"æ£€æŸ¥ç‚¹ä¿å­˜åˆ°: {checkpoint_path}")

    def save_history(self):
        """ä¿å­˜è®­ç»ƒå†å²"""
        history_path = self.output_dir / "training_history.json"

        import json
        with open(history_path, 'w') as f:
            json.dump(self.history, f, indent=2)

        # ä¿å­˜ä¸ºnumpyæ ¼å¼
        np.save(self.output_dir / "history.npy", self.history)

    def visualize_results(self, test_results: Dict):
        """å¯è§†åŒ–ç»“æœ"""
        try:
            import matplotlib.pyplot as plt
            import seaborn as sns

            viz_dir = self.output_dir / "visualizations"
            viz_dir.mkdir(parents=True, exist_ok=True)

            # 1. è®­ç»ƒå†å²
            fig, axes = plt.subplots(2, 2, figsize=(15, 10))

            # æŸå¤±æ›²çº¿
            axes[0, 0].plot(self.history['train_loss'], label='è®­ç»ƒæŸå¤±')
            axes[0, 0].plot(self.history['val_loss'], label='éªŒè¯æŸå¤±')
            axes[0, 0].set_xlabel('Epoch')
            axes[0, 0].set_ylabel('æŸå¤±')
            axes[0, 0].set_title('è®­ç»ƒå’ŒéªŒè¯æŸå¤±')
            axes[0, 0].legend()
            axes[0, 0].grid(True)

            # å‡†ç¡®ç‡æ›²çº¿
            axes[0, 1].plot(self.history['train_acc'], label='è®­ç»ƒå‡†ç¡®ç‡')
            axes[0, 1].plot(self.history['val_acc'], label='éªŒè¯å‡†ç¡®ç‡')
            axes[0, 1].set_xlabel('Epoch')
            axes[0, 1].set_ylabel('å‡†ç¡®ç‡ (%)')
            axes[0, 1].set_title('è®­ç»ƒå’ŒéªŒè¯å‡†ç¡®ç‡')
            axes[0, 1].legend()
            axes[0, 1].grid(True)

            # å­¦ä¹ ç‡æ›²çº¿
            axes[1, 0].plot(self.history['lr'])
            axes[1, 0].set_xlabel('Epoch')
            axes[1, 0].set_ylabel('å­¦ä¹ ç‡')
            axes[1, 0].set_title('å­¦ä¹ ç‡å˜åŒ–')
            axes[1, 0].grid(True)

            # æ··æ·†çŸ©é˜µ
            cm = test_results['confusion_matrix']
            soil_types = test_results.get('soil_types', ['Class 0', 'Class 1', 'Class 2', 'Class 3'][:len(cm)])

            im = axes[1, 1].imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)
            axes[1, 1].set_title('æ··æ·†çŸ©é˜µ')
            plt.colorbar(im, ax=axes[1, 1])

            # è®¾ç½®åˆ»åº¦
            tick_marks = np.arange(len(soil_types))
            axes[1, 1].set_xticks(tick_marks)
            axes[1, 1].set_xticklabels(soil_types, rotation=45)
            axes[1, 1].set_yticks(tick_marks)
            axes[1, 1].set_yticklabels(soil_types)

            # æ·»åŠ æ•°å€¼
            thresh = cm.max() / 2.
            for i in range(cm.shape[0]):
                for j in range(cm.shape[1]):
                    axes[1, 1].text(j, i, format(cm[i, j], 'd'),
                                  horizontalalignment="center",
                                  color="white" if cm[i, j] > thresh else "black")

            plt.tight_layout()
            plt.savefig(viz_dir / "training_results.png", dpi=300, bbox_inches='tight')
            plt.close()

            # 2. ç‰¹å¾å¯è§†åŒ–ï¼ˆt-SNEï¼‰
            try:
                from sklearn.manifold import TSNE

                features = test_results['features']
                labels = test_results['labels']

                if len(features) > 50:
                    # é‡‡æ ·
                    n_samples = min(500, len(features))
                    indices = np.random.choice(len(features), n_samples, replace=False)
                    features_sample = features[indices]
                    labels_sample = [labels[i] for i in indices]

                    # t-SNE
                    tsne = TSNE(n_components=2, random_state=42, perplexity=30)
                    features_2d = tsne.fit_transform(features_sample)

                    # ç»˜åˆ¶
                    plt.figure(figsize=(10, 8))
                    scatter = plt.scatter(features_2d[:, 0], features_2d[:, 1],
                                        c=labels_sample, cmap='tab10', alpha=0.7, s=50)

                    plt.colorbar(scatter, label='åœŸå£¤ç±»å‹')
                    plt.xlabel('t-SNE ç»´åº¦ 1')
                    plt.ylabel('t-SNE ç»´åº¦ 2')
                    plt.title('åœŸå£¤ç±»å‹ç‰¹å¾ç©ºé—´å¯è§†åŒ– (t-SNE)')
                    plt.tight_layout()
                    plt.savefig(viz_dir / "feature_tsne.png", dpi=300, bbox_inches='tight')
                    plt.close()
            except Exception as e:
                self.logger.warning(f"t-SNEå¯è§†åŒ–å¤±è´¥: {e}")

            self.logger.info(f"å¯è§†åŒ–ç»“æœä¿å­˜åˆ°: {viz_dir}")

        except ImportError as e:
            self.logger.warning(f"å¯è§†åŒ–ä¾èµ–åº“æœªå®‰è£…: {e}")

    def run_pipeline(self):
        """è¿è¡Œå®Œæ•´å¾®è°ƒæµæ°´çº¿"""
        self.logger.info("=" * 60)
        self.logger.info("ğŸŒ± åœŸå£¤ç±»å‹å¾®è°ƒæµæ°´çº¿")
        self.logger.info("=" * 60)

        try:
            # 1. å‡†å¤‡æ•°æ®
            self.logger.info("æ­¥éª¤1: å‡†å¤‡æ•°æ®...")
            dataloaders = self.data_loader.create_dataloaders()

            if 'train' not in dataloaders or 'val' not in dataloaders:
                raise ValueError("ç¼ºå°‘è®­ç»ƒé›†æˆ–éªŒè¯é›†")

            # 2. æ„å»ºæ¨¡å‹
            self.logger.info("æ­¥éª¤2: æ„å»ºæ¨¡å‹...")
            self.build_model(self.config.paths["eurosat_model"])

            # 3. è®­ç»ƒæ¨¡å‹
            self.logger.info("æ­¥éª¤3: è®­ç»ƒæ¨¡å‹...")
            history = self.train(dataloaders)

            # 4. æµ‹è¯•æ¨¡å‹
            self.logger.info("æ­¥éª¤4: æµ‹è¯•æ¨¡å‹...")
            if 'test' in dataloaders:
                test_dataset = dataloaders['test'].dataset
                test_results = self.test(dataloaders['test'], test_dataset)

                self.logger.info(f"æµ‹è¯•å‡†ç¡®ç‡: {test_results['accuracy']:.4f}")

                # ä¿å­˜æµ‹è¯•ç»“æœ
                test_path = self.output_dir / "test_results.pkl"
                with open(test_path, 'wb') as f:
                    import pickle
                    pickle.dump(test_results, f)

                self.logger.info(f"æµ‹è¯•ç»“æœä¿å­˜åˆ°: {test_path}")

                # 5. å¯è§†åŒ–
                self.logger.info("æ­¥éª¤5: å¯è§†åŒ–...")
                self.visualize_results(test_results)

            self.logger.info("=" * 60)
            self.logger.info("âœ… åœŸå£¤ç±»å‹å¾®è°ƒå®Œæˆ!")
            self.logger.info("=" * 60)

            return {
                'model': self.model,
                'history': history,
                'test_results': test_results if 'test' in dataloaders else None
            }

        except Exception as e:
            self.logger.error(f"å¾®è°ƒå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            raise


def main():
    """ä¸»å‡½æ•°"""
    # åˆ›å»ºå¾®è°ƒå™¨
    finetuner = SoilFinetuner()

    # è¿è¡Œå¾®è°ƒæµæ°´çº¿
    results = finetuner.run_pipeline()

    return results


if __name__ == "__main__":
    main()