"""
æ°”è±¡é¢„æµ‹æ¨¡å‹å¾®è°ƒ - ä¿®å¤ç»´åº¦ç‰ˆæœ¬
è‡ªåŠ¨æ£€æµ‹è¾“å…¥ç‰¹å¾ç»´åº¦
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import numpy as np
import pickle
from pathlib import Path
from typing import Dict, List, Tuple, Optional
import warnings

warnings.filterwarnings('ignore')

from config import config


class WeatherDataset(Dataset):
    """æ°”è±¡æ—¶åºæ•°æ®é›†"""

    def __init__(self, features: np.ndarray, targets: np.ndarray,
                 window_size: int = 7, forecast_horizon: int = 1):
        """
        åˆå§‹åŒ–æ•°æ®é›†

        Args:
            features: ç‰¹å¾æ•°ç»„ [n_samples, n_timesteps, n_features]
            targets: ç›®æ ‡æ•°ç»„ [n_samples, 2] (Next_Tmax, Next_Tmin)
            window_size: æ—¶é—´çª—å£å¤§å°
            forecast_horizon: é¢„æµ‹æ­¥é•¿
        """
        self.features = features.astype(np.float32)
        self.targets = targets.astype(np.float32)
        self.window_size = window_size
        self.forecast_horizon = forecast_horizon
        self.feature_dim = features.shape[-1] if len(features.shape) > 2 else features.shape[1]

        # éªŒè¯æ•°æ®å½¢çŠ¶
        assert len(self.features) == len(self.targets), "ç‰¹å¾å’Œç›®æ ‡é•¿åº¦ä¸ä¸€è‡´"

        print(f"æ°”è±¡æ•°æ®é›†: {len(self.features)} æ ·æœ¬")
        print(f"ç‰¹å¾å½¢çŠ¶: {self.features.shape}")
        print(f"ç›®æ ‡å½¢çŠ¶: {self.targets.shape}")
        print(f"ç‰¹å¾ç»´åº¦: {self.feature_dim}")

    def __len__(self):
        return len(self.features)

    def __getitem__(self, idx):
        # è·å–çª—å£ç‰¹å¾
        window_features = self.features[idx]

        # è·å–ç›®æ ‡
        window_targets = self.targets[idx]

        # è½¬æ¢ä¸ºå¼ é‡
        features_tensor = torch.from_numpy(window_features)
        targets_tensor = torch.from_numpy(window_targets)

        return {
            'features': features_tensor,
            'targets': targets_tensor,
            'idx': idx,
            'feature_dim': self.feature_dim
        }


class WeatherPredictor(nn.Module):
    """æ°”è±¡é¢„æµ‹æ¨¡å‹ï¼ˆLSTM + Attentionï¼‰"""

    def __init__(self, input_size: int = None, hidden_size: int = 128,
                 num_layers: int = 2, output_size: int = 2, dropout: float = 0.2):
        super(WeatherPredictor, self).__init__()

        self.input_size = input_size
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.output_size = output_size

        # å¦‚æœæœªæŒ‡å®šinput_sizeï¼Œå»¶è¿Ÿåˆ›å»ºLSTM
        self.lstm = None
        if input_size is not None:
            self._build_lstm(input_size)

        # æ³¨æ„åŠ›æœºåˆ¶
        self.attention = nn.Sequential(
            nn.Linear(hidden_size * 2, hidden_size),
            nn.Tanh(),
            nn.Linear(hidden_size, 1)
        )

        # å…¨è¿æ¥å±‚
        self.fc = nn.Sequential(
            nn.Linear(hidden_size * 2, hidden_size),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_size, hidden_size // 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_size // 2, output_size)
        )

        # å±‚å½’ä¸€åŒ–
        self.ln = nn.LayerNorm(hidden_size * 2)

    def _build_lstm(self, input_size: int):
        """æ„å»ºLSTMå±‚"""
        self.input_size = input_size
        self.lstm = nn.LSTM(
            input_size=input_size,
            hidden_size=self.hidden_size,
            num_layers=self.num_layers,
            batch_first=True,
            dropout=0.2 if self.num_layers > 1 else 0,
            bidirectional=True
        )

    def forward(self, x: torch.Tensor, return_attention: bool = False):
        """
        å‰å‘ä¼ æ’­

        Args:
            x: è¾“å…¥åºåˆ— [batch_size, seq_len, input_size]
            return_attention: æ˜¯å¦è¿”å›æ³¨æ„åŠ›æƒé‡

        Returns:
            é¢„æµ‹ç»“æœæˆ–(é¢„æµ‹ç»“æœ, æ³¨æ„åŠ›æƒé‡)
        """
        if self.lstm is None:
            # åŠ¨æ€æ„å»ºLSTM
            input_size = x.size(-1)
            self._build_lstm(input_size)
            self.lstm = self.lstm.to(x.device)

        batch_size = x.size(0)

        # éªŒè¯è¾“å…¥ç»´åº¦
        if x.size(-1) != self.input_size:
            raise ValueError(f"è¾“å…¥ç»´åº¦ä¸åŒ¹é…: æœŸæœ› {self.input_size}, å®é™… {x.size(-1)}")

        # LSTMç¼–ç 
        lstm_out, (hidden, cell) = self.lstm(x)  # [batch, seq_len, hidden*2]
        lstm_out = self.ln(lstm_out)

        # æ³¨æ„åŠ›æƒé‡
        attention_weights = torch.softmax(
            self.attention(lstm_out).squeeze(-1), dim=1
        ).unsqueeze(2)  # [batch, seq_len, 1]

        # ä¸Šä¸‹æ–‡å‘é‡
        context = torch.sum(lstm_out * attention_weights, dim=1)  # [batch, hidden*2]

        # é¢„æµ‹
        predictions = self.fc(context)  # [batch, output_size]

        if return_attention:
            return predictions, attention_weights.squeeze()
        return predictions

    def extract_features(self, x: torch.Tensor) -> torch.Tensor:
        """æå–ç‰¹å¾"""
        if self.lstm is None:
            input_size = x.size(-1)
            self._build_lstm(input_size)
            self.lstm = self.lstm.to(x.device)

        lstm_out, _ = self.lstm(x)
        lstm_out = self.ln(lstm_out)

        # æ³¨æ„åŠ›æƒé‡
        attention_weights = torch.softmax(
            self.attention(lstm_out).squeeze(-1), dim=1
        ).unsqueeze(2)

        # ä¸Šä¸‹æ–‡å‘é‡
        context = torch.sum(lstm_out * attention_weights, dim=1)

        return context


class WeatherDataLoader:
    """æ°”è±¡æ•°æ®åŠ è½½å™¨ - ä¿®å¤ç‰ˆæœ¬"""

    def __init__(self):
        self.config = config
        self.feature_dim = None

    def load_weather_samples(self) -> Dict[str, np.ndarray]:
        """åŠ è½½æ°”è±¡æ—¶åºæ ·æœ¬"""
        print("åŠ è½½æ°”è±¡æ—¶åºæ ·æœ¬...")

        samples_path = Path(self.config.paths["weather_ts_samples"])

        if not samples_path.exists():
            raise FileNotFoundError(f"æ°”è±¡æ ·æœ¬æ–‡ä»¶ä¸å­˜åœ¨: {samples_path}")

        with open(samples_path, 'rb') as f:
            samples = pickle.load(f)

        # æå–ç‰¹å¾å’Œç›®æ ‡
        features = []
        targets = []

        for sample in samples:
            # ç‰¹å¾: [window_size, feature_dim]
            window_features = sample['features']
            features.append(window_features)

            # ç›®æ ‡: [2] (Next_Tmax, Next_Tmin)
            target_tmax = sample['targets']['Next_Tmax']
            target_tmin = sample['targets']['Next_Tmin']
            targets.append([target_tmax, target_tmin])

        features = np.array(features)
        targets = np.array(targets)

        # è·å–å®é™…ç‰¹å¾ç»´åº¦
        if len(features.shape) == 3:
            self.feature_dim = features.shape[-1]
        else:
            self.feature_dim = features.shape[1] if len(features.shape) > 1 else 1

        print(f"åŠ è½½ {len(features)} ä¸ªæ°”è±¡æ ·æœ¬")
        print(f"ç‰¹å¾å½¢çŠ¶: {features.shape}")
        print(f"ç›®æ ‡å½¢çŠ¶: {targets.shape}")
        print(f"å®é™…ç‰¹å¾ç»´åº¦: {self.feature_dim}")

        # æ›´æ–°é…ç½®ä¸­çš„ç‰¹å¾ç»´åº¦
        if self.feature_dim != self.config.weather_config["input_features"]:
            print(f"æ›´æ–°é…ç½®ç‰¹å¾ç»´åº¦: {self.config.weather_config['input_features']} -> {self.feature_dim}")
            self.config.weather_config["input_features"] = self.feature_dim

        return {
            'features': features,
            'targets': targets,
            'feature_dim': self.feature_dim
        }

    def create_datasets(self, split_ratio: Tuple = (0.7, 0.15, 0.15)) -> Dict[str, WeatherDataset]:
        """åˆ›å»ºæ•°æ®é›†"""
        data = self.load_weather_samples()

        n_samples = len(data['features'])
        indices = np.random.permutation(n_samples)

        train_size = int(n_samples * split_ratio[0])
        val_size = int(n_samples * split_ratio[1])

        train_idx = indices[:train_size]
        val_idx = indices[train_size:train_size + val_size]
        test_idx = indices[train_size + val_size:]

        datasets = {}

        for name, idx in [('train', train_idx), ('val', val_idx), ('test', test_idx)]:
            datasets[name] = WeatherDataset(
                features=data['features'][idx],
                targets=data['targets'][idx],
                window_size=self.config.weather_config["window_size"]
            )

        print(f"æ•°æ®é›†åˆ’åˆ†: è®­ç»ƒé›†={len(datasets['train'])}, "
              f"éªŒè¯é›†={len(datasets['val'])}, æµ‹è¯•é›†={len(datasets['test'])}")

        return datasets

    def create_dataloaders(self, batch_size: int = None) -> Dict[str, DataLoader]:
        """åˆ›å»ºæ•°æ®åŠ è½½å™¨"""
        if batch_size is None:
            batch_size = self.config.training_config["batch_size"]

        datasets = self.create_datasets()
        dataloaders = {}

        for split in ['train', 'val', 'test']:
            if split in datasets:
                shuffle = (split == 'train')
                dataloaders[split] = DataLoader(
                    datasets[split],
                    batch_size=batch_size,
                    shuffle=shuffle,
                    num_workers=4,
                    pin_memory=True
                )

        return dataloaders


class WeatherFinetuner:
    """æ°”è±¡é¢„æµ‹å¾®è°ƒå™¨ - ä¿®å¤ç‰ˆæœ¬"""

    def __init__(self):
        self.config = config
        self.device = self.config.get_device()

        # æ•°æ®åŠ è½½å™¨
        self.data_loader = WeatherDataLoader()

        # æ¨¡å‹
        self.model = None
        self.criterion = None
        self.optimizer = None
        self.scheduler = None

        # è®­ç»ƒå†å²
        self.history = {
            'train_loss': [], 'train_mae': [], 'train_rmse': [],
            'val_loss': [], 'val_mae': [], 'val_rmse': [],
            'lr': []
        }

        # è¾“å‡ºç›®å½•
        self.output_dir = Path(self.config.paths["finetune_output"]) / "weather_predictor"
        self.output_dir.mkdir(parents=True, exist_ok=True)

        # æ—¥å¿—
        self._setup_logging()

        print(self.config)

    def _setup_logging(self):
        """è®¾ç½®æ—¥å¿—"""
        import logging

        log_file = self.output_dir / "training.log"

        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_file),
                logging.StreamHandler()
            ]
        )

        self.logger = logging.getLogger(__name__)

    def analyze_features(self, dataloader: DataLoader):
        """åˆ†æç‰¹å¾ç»´åº¦"""
        print("åˆ†æç‰¹å¾ç»´åº¦...")

        for batch in dataloader:
            features = batch['features']
            print(f"æ‰¹æ¬¡ç‰¹å¾å½¢çŠ¶: {features.shape}")

            # è·å–å®é™…ç»´åº¦
            actual_feature_dim = features.shape[-1]
            config_feature_dim = self.config.weather_config["input_features"]

            if actual_feature_dim != config_feature_dim:
                print(f"ç‰¹å¾ç»´åº¦ä¸åŒ¹é…: é…ç½®={config_feature_dim}, å®é™…={actual_feature_dim}")
                print(f"æ›´æ–°é…ç½®...")

                # æ›´æ–°é…ç½®
                self.config.weather_config["input_features"] = actual_feature_dim

                # ä¿å­˜æ›´æ–°åçš„é…ç½®
                config.save()

            break

        return self.config.weather_config["input_features"]

    def build_model(self, input_size: int = None) -> nn.Module:
        """æ„å»ºæ¨¡å‹"""
        print("æ„å»ºæ°”è±¡é¢„æµ‹æ¨¡å‹...")

        weather_config = self.config.weather_config

        # ä½¿ç”¨å®é™…ç‰¹å¾ç»´åº¦
        if input_size is None:
            input_size = weather_config["input_features"]

        print(f"ä½¿ç”¨è¾“å…¥ç‰¹å¾ç»´åº¦: {input_size}")

        self.model = WeatherPredictor(
            input_size=input_size,
            hidden_size=weather_config["hidden_size"],
            num_layers=weather_config["num_layers"],
            output_size=weather_config["output_features"],
            dropout=0.2
        )

        # ç§»åŠ¨åˆ°è®¾å¤‡
        self.model = self.model.to(self.device)

        # æŸå¤±å‡½æ•° (HuberæŸå¤±å¯¹å¼‚å¸¸å€¼æ›´é²æ£’)
        self.criterion = nn.HuberLoss(delta=1.0)

        # ä¼˜åŒ–å™¨
        self.optimizer = optim.AdamW(
            self.model.parameters(),
            lr=self.config.training_config["learning_rate"],
            weight_decay=self.config.training_config["weight_decay"]
        )

        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        self.scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(
            self.optimizer,
            T_0=10,
            T_mult=2,
            eta_min=1e-6
        )

        print(f"æ¨¡å‹å‚æ•°é‡: {sum(p.numel() for p in self.model.parameters())}")

        return self.model

    def calculate_metrics(self, predictions: torch.Tensor, targets: torch.Tensor) -> Dict:
        """è®¡ç®—è¯„ä¼°æŒ‡æ ‡"""
        predictions_np = predictions.cpu().detach().numpy()
        targets_np = targets.cpu().detach().numpy()

        # MAE
        mae = np.mean(np.abs(predictions_np - targets_np))

        # RMSE
        rmse = np.sqrt(np.mean((predictions_np - targets_np) ** 2))

        # RÂ² Score
        ss_res = np.sum((targets_np - predictions_np) ** 2)
        ss_tot = np.sum((targets_np - np.mean(targets_np)) ** 2)
        r2 = 1 - (ss_res / (ss_tot + 1e-8))

        return {
            'mae': float(mae),
            'rmse': float(rmse),
            'r2': float(r2)
        }

    def train_epoch(self, dataloader: DataLoader, epoch: int) -> Tuple[float, Dict]:
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        total_loss = 0
        total_mae = 0
        total_rmse = 0

        from tqdm import tqdm
        pbar = tqdm(dataloader, desc=f'Epoch {epoch + 1} [è®­ç»ƒ]')

        for batch_idx, batch in enumerate(pbar):
            features = batch['features'].to(self.device, non_blocking=True)
            targets = batch['targets'].to(self.device, non_blocking=True)

            # éªŒè¯ç‰¹å¾ç»´åº¦
            expected_dim = self.config.weather_config["input_features"]
            actual_dim = features.shape[-1]

            if actual_dim != expected_dim:
                print(f"è­¦å‘Š: ç‰¹å¾ç»´åº¦ä¸åŒ¹é…! æ‰¹æ¬¡ {batch_idx}: æœŸæœ› {expected_dim}, å®é™… {actual_dim}")
                # è°ƒæ•´æ¨¡å‹è¾“å…¥ç»´åº¦
                if hasattr(self.model, '_build_lstm'):
                    self.model._build_lstm(actual_dim)
                    self.model.lstm = self.model.lstm.to(self.device)

            # å‰å‘ä¼ æ’­
            self.optimizer.zero_grad(set_to_none=True)
            predictions = self.model(features)
            loss = self.criterion(predictions, targets)

            # åå‘ä¼ æ’­
            loss.backward()

            # æ¢¯åº¦è£å‰ª
            torch.nn.utils.clip_grad_norm_(
                self.model.parameters(),
                self.config.training_config["gradient_clip"]
            )

            self.optimizer.step()

            # è®¡ç®—æŒ‡æ ‡
            metrics = self.calculate_metrics(predictions, targets)

            # ç»Ÿè®¡
            total_loss += loss.item()
            total_mae += metrics['mae']
            total_rmse += metrics['rmse']

            # æ›´æ–°è¿›åº¦æ¡
            pbar.set_postfix({
                'loss': f'{loss.item():.4f}',
                'mae': f'{metrics["mae"]:.3f}',
                'rmse': f'{metrics["rmse"]:.3f}'
            })

        avg_loss = total_loss / len(dataloader)
        avg_mae = total_mae / len(dataloader)
        avg_rmse = total_rmse / len(dataloader)

        return avg_loss, {'mae': avg_mae, 'rmse': avg_rmse}

    def validate(self, dataloader: DataLoader, epoch: int) -> Tuple[float, Dict]:
        """éªŒè¯"""
        self.model.eval()
        total_loss = 0
        total_mae = 0
        total_rmse = 0

        all_predictions = []
        all_targets = []

        with torch.no_grad():
            from tqdm import tqdm
            pbar = tqdm(dataloader, desc=f'Epoch {epoch + 1} [éªŒè¯]')

            for batch_idx, batch in enumerate(pbar):
                features = batch['features'].to(self.device, non_blocking=True)
                targets = batch['targets'].to(self.device, non_blocking=True)

                # éªŒè¯ç‰¹å¾ç»´åº¦
                expected_dim = self.config.weather_config["input_features"]
                actual_dim = features.shape[-1]

                if actual_dim != expected_dim:
                    print(f"è­¦å‘Š: éªŒè¯é›†ç‰¹å¾ç»´åº¦ä¸åŒ¹é…! æ‰¹æ¬¡ {batch_idx}: æœŸæœ› {expected_dim}, å®é™… {actual_dim}")

                # å‰å‘ä¼ æ’­
                predictions = self.model(features)
                loss = self.criterion(predictions, targets)

                # è®¡ç®—æŒ‡æ ‡
                metrics = self.calculate_metrics(predictions, targets)

                # ç»Ÿè®¡
                total_loss += loss.item()
                total_mae += metrics['mae']
                total_rmse += metrics['rmse']

                # æ”¶é›†ç»“æœ
                all_predictions.extend(predictions.cpu().numpy())
                all_targets.extend(targets.cpu().numpy())

                # æ›´æ–°è¿›åº¦æ¡
                pbar.set_postfix({
                    'loss': f'{loss.item():.4f}',
                    'mae': f'{metrics["mae"]:.3f}',
                    'rmse': f'{metrics["rmse"]:.3f}'
                })

        avg_loss = total_loss / len(dataloader)
        avg_mae = total_mae / len(dataloader)
        avg_rmse = total_rmse / len(dataloader)

        return avg_loss, {'mae': avg_mae, 'rmse': avg_rmse}, all_predictions, all_targets

    def test(self, dataloader: DataLoader) -> Dict:
        """æµ‹è¯•"""
        self.model.eval()

        all_predictions = []
        all_targets = []
        all_features = []

        with torch.no_grad():
            from tqdm import tqdm
            for batch in tqdm(dataloader, desc="æµ‹è¯•"):
                features = batch['features'].to(self.device)
                targets = batch['targets'].numpy()

                # å‰å‘ä¼ æ’­
                predictions = self.model(features)

                all_predictions.extend(predictions.cpu().numpy())
                all_targets.extend(targets)
                all_features.extend(features.cpu().numpy())

        all_predictions = np.array(all_predictions)
        all_targets = np.array(all_targets)

        # è®¡ç®—è¯¦ç»†æŒ‡æ ‡
        metrics = {}

        # æ•´ä½“æŒ‡æ ‡
        metrics['mae'] = np.mean(np.abs(all_predictions - all_targets))
        metrics['rmse'] = np.sqrt(np.mean((all_predictions - all_targets) ** 2))

        # åˆ†é¡¹æŒ‡æ ‡ (Tmax, Tmin)
        for i, name in enumerate(['Tmax', 'Tmin']):
            pred = all_predictions[:, i]
            true = all_targets[:, i]

            mae = np.mean(np.abs(pred - true))
            rmse = np.sqrt(np.mean((pred - true) ** 2))
            r2 = 1 - np.sum((true - pred) ** 2) / np.sum((true - np.mean(true)) ** 2)

            metrics[f'{name}_mae'] = float(mae)
            metrics[f'{name}_rmse'] = float(rmse)
            metrics[f'{name}_r2'] = float(r2)

        return {
            'metrics': metrics,
            'predictions': all_predictions,
            'targets': all_targets,
            'features': np.array(all_features)
        }

    def train(self, dataloaders: Dict[str, DataLoader]):
        """è®­ç»ƒæ¨¡å‹"""
        print("å¼€å§‹è®­ç»ƒæ°”è±¡é¢„æµ‹æ¨¡å‹...")

        best_val_loss = float('inf')
        patience_counter = 0
        best_model_state = None

        epochs = self.config.training_config["epochs"]
        patience = self.config.training_config["patience"]

        for epoch in range(epochs):
            # è®­ç»ƒ
            train_loss, train_metrics = self.train_epoch(dataloaders['train'], epoch)

            # éªŒè¯
            val_loss, val_metrics, val_preds, val_targets = self.validate(dataloaders['val'], epoch)

            # æ›´æ–°å­¦ä¹ ç‡
            if self.scheduler is not None:
                self.scheduler.step()

            # è®°å½•å†å²
            current_lr = self.optimizer.param_groups[0]['lr']
            self.history['train_loss'].append(train_loss)
            self.history['train_mae'].append(train_metrics['mae'])
            self.history['train_rmse'].append(train_metrics['rmse'])
            self.history['val_loss'].append(val_loss)
            self.history['val_mae'].append(val_metrics['mae'])
            self.history['val_rmse'].append(val_metrics['rmse'])
            self.history['lr'].append(current_lr)

            self.logger.info(
                f"Epoch {epoch + 1}/{epochs}: "
                f"Train Loss: {train_loss:.4f}, MAE: {train_metrics['mae']:.3f}, RMSE: {train_metrics['rmse']:.3f} | "
                f"Val Loss: {val_loss:.4f}, MAE: {val_metrics['mae']:.3f}, RMSE: {val_metrics['rmse']:.3f} | "
                f"LR: {current_lr:.6f}"
            )

            # æ—©åœå’Œä¿å­˜æœ€ä½³æ¨¡å‹
            if val_loss < best_val_loss:
                best_val_loss = val_loss
                patience_counter = 0
                best_model_state = self.model.state_dict().copy()

                # ä¿å­˜æœ€ä½³æ¨¡å‹
                self.save_checkpoint(epoch, 'best')
                self.logger.info(f"æœ€ä½³æ¨¡å‹ä¿å­˜ï¼ŒéªŒè¯æŸå¤±: {val_loss:.4f}")
            else:
                patience_counter += 1
                if patience_counter >= patience:
                    self.logger.info(f"æ—©åœè§¦å‘ï¼Œåœ¨epoch {epoch + 1}")
                    break

        # åŠ è½½æœ€ä½³æ¨¡å‹
        if best_model_state is not None:
            self.model.load_state_dict(best_model_state)

        # ä¿å­˜æœ€ç»ˆæ¨¡å‹
        self.save_checkpoint(epochs - 1, 'final')

        # ä¿å­˜è®­ç»ƒå†å²
        self.save_history()

        self.logger.info(f"è®­ç»ƒå®Œæˆï¼Œæœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.4f}")

        return self.history

    def save_checkpoint(self, epoch: int, name: str):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        checkpoint_path = self.output_dir / f"{name}_checkpoint.pth"

        torch.save({
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict() if self.scheduler else None,
            'history': self.history,
            'config': self.config,
            'input_size': self.config.weather_config["input_features"]
        }, checkpoint_path)

        self.logger.info(f"æ£€æŸ¥ç‚¹ä¿å­˜åˆ°: {checkpoint_path}")

    def save_history(self):
        """ä¿å­˜è®­ç»ƒå†å²"""
        history_path = self.output_dir / "training_history.json"

        import json
        with open(history_path, 'w') as f:
            json.dump(self.history, f, indent=2)

        np.save(self.output_dir / "history.npy", self.history)

    def visualize_results(self, test_results: Dict):
        """å¯è§†åŒ–ç»“æœ"""
        try:
            import matplotlib.pyplot as plt
            import seaborn as sns

            viz_dir = self.output_dir / "visualizations"
            viz_dir.mkdir(parents=True, exist_ok=True)

            # 1. è®­ç»ƒå†å²
            fig, axes = plt.subplots(2, 3, figsize=(18, 10))

            # æŸå¤±æ›²çº¿
            axes[0, 0].plot(self.history['train_loss'], label='è®­ç»ƒæŸå¤±')
            axes[0, 0].plot(self.history['val_loss'], label='éªŒè¯æŸå¤±')
            axes[0, 0].set_xlabel('Epoch')
            axes[0, 0].set_ylabel('æŸå¤±')
            axes[0, 0].set_title('è®­ç»ƒå’ŒéªŒè¯æŸå¤±')
            axes[0, 0].legend()
            axes[0, 0].grid(True)

            # MAEæ›²çº¿
            axes[0, 1].plot(self.history['train_mae'], label='è®­ç»ƒMAE')
            axes[0, 1].plot(self.history['val_mae'], label='éªŒè¯MAE')
            axes[0, 1].set_xlabel('Epoch')
            axes[0, 1].set_ylabel('MAE (Â°C)')
            axes[0, 1].set_title('å¹³å‡ç»å¯¹è¯¯å·®')
            axes[0, 1].legend()
            axes[0, 1].grid(True)

            # RMSEæ›²çº¿
            axes[0, 2].plot(self.history['train_rmse'], label='è®­ç»ƒRMSE')
            axes[0, 2].plot(self.history['val_rmse'], label='éªŒè¯RMSE')
            axes[0, 2].set_xlabel('Epoch')
            axes[0, 2].set_ylabel('RMSE (Â°C)')
            axes[0, 2].set_title('å‡æ–¹æ ¹è¯¯å·®')
            axes[0, 2].legend()
            axes[0, 2].grid(True)

            # å­¦ä¹ ç‡æ›²çº¿
            axes[1, 0].plot(self.history['lr'])
            axes[1, 0].set_xlabel('Epoch')
            axes[1, 0].set_ylabel('å­¦ä¹ ç‡')
            axes[1, 0].set_title('å­¦ä¹ ç‡å˜åŒ–')
            axes[1, 0].grid(True)

            # é¢„æµ‹ vs çœŸå®å€¼ (Tmax)
            predictions = test_results['predictions']
            targets = test_results['targets']

            # Tmaxæ•£ç‚¹å›¾
            axes[1, 1].scatter(targets[:, 0], predictions[:, 0], alpha=0.5, s=10)
            axes[1, 1].plot([targets[:, 0].min(), targets[:, 0].max()],
                            [targets[:, 0].min(), targets[:, 0].max()], 'r--')
            axes[1, 1].set_xlabel('çœŸå® Tmax (Â°C)')
            axes[1, 1].set_ylabel('é¢„æµ‹ Tmax (Â°C)')
            axes[1, 1].set_title('æœ€é«˜æ¸©åº¦é¢„æµ‹')
            axes[1, 1].grid(True)

            # Tminæ•£ç‚¹å›¾
            axes[1, 2].scatter(targets[:, 1], predictions[:, 1], alpha=0.5, s=10)
            axes[1, 2].plot([targets[:, 1].min(), targets[:, 1].max()],
                            [targets[:, 1].min(), targets[:, 1].max()], 'r--')
            axes[1, 2].set_xlabel('çœŸå® Tmin (Â°C)')
            axes[1, 2].set_ylabel('é¢„æµ‹ Tmin (Â°C)')
            axes[1, 2].set_title('æœ€ä½æ¸©åº¦é¢„æµ‹')
            axes[1, 2].grid(True)

            plt.tight_layout()
            plt.savefig(viz_dir / "training_results.png", dpi=300, bbox_inches='tight')
            plt.close()

            # 2. è¯¯å·®åˆ†å¸ƒ
            fig, axes = plt.subplots(1, 2, figsize=(12, 5))

            # Tmaxè¯¯å·®åˆ†å¸ƒ
            tmax_errors = predictions[:, 0] - targets[:, 0]
            axes[0].hist(tmax_errors, bins=50, alpha=0.7, edgecolor='black')
            axes[0].axvline(x=0, color='r', linestyle='--')
            axes[0].set_xlabel('é¢„æµ‹è¯¯å·® (Â°C)')
            axes[0].set_ylabel('é¢‘æ•°')
            axes[0].set_title('Tmaxé¢„æµ‹è¯¯å·®åˆ†å¸ƒ')
            axes[0].grid(True, alpha=0.3)

            # Tminè¯¯å·®åˆ†å¸ƒ
            tmin_errors = predictions[:, 1] - targets[:, 1]
            axes[1].hist(tmin_errors, bins=50, alpha=0.7, edgecolor='black', color='orange')
            axes[1].axvline(x=0, color='r', linestyle='--')
            axes[1].set_xlabel('é¢„æµ‹è¯¯å·® (Â°C)')
            axes[1].set_ylabel('é¢‘æ•°')
            axes[1].set_title('Tminé¢„æµ‹è¯¯å·®åˆ†å¸ƒ')
            axes[1].grid(True, alpha=0.3)

            plt.tight_layout()
            plt.savefig(viz_dir / "error_distribution.png", dpi=300, bbox_inches='tight')
            plt.close()

            self.logger.info(f"å¯è§†åŒ–ç»“æœä¿å­˜åˆ°: {viz_dir}")

        except ImportError as e:
            self.logger.warning(f"å¯è§†åŒ–ä¾èµ–åº“æœªå®‰è£…: {e}")

    def run_pipeline(self):
        """è¿è¡Œå®Œæ•´å¾®è°ƒæµæ°´çº¿"""
        self.logger.info("=" * 60)
        self.logger.info("ğŸŒ¤ï¸ æ°”è±¡é¢„æµ‹å¾®è°ƒæµæ°´çº¿")
        self.logger.info("=" * 60)

        try:
            # 1. å‡†å¤‡æ•°æ®
            self.logger.info("æ­¥éª¤1: å‡†å¤‡æ•°æ®...")
            dataloaders = self.data_loader.create_dataloaders()

            if 'train' not in dataloaders or 'val' not in dataloaders:
                raise ValueError("ç¼ºå°‘è®­ç»ƒé›†æˆ–éªŒè¯é›†")

            # 2. åˆ†æç‰¹å¾ç»´åº¦
            self.logger.info("æ­¥éª¤2: åˆ†æç‰¹å¾ç»´åº¦...")
            actual_feature_dim = self.analyze_features(dataloaders['train'])

            # 3. æ„å»ºæ¨¡å‹
            self.logger.info("æ­¥éª¤3: æ„å»ºæ¨¡å‹...")
            self.build_model(actual_feature_dim)

            # 4. è®­ç»ƒæ¨¡å‹
            self.logger.info("æ­¥éª¤4: è®­ç»ƒæ¨¡å‹...")
            history = self.train(dataloaders)

            # 5. æµ‹è¯•æ¨¡å‹
            self.logger.info("æ­¥éª¤5: æµ‹è¯•æ¨¡å‹...")
            if 'test' in dataloaders:
                test_results = self.test(dataloaders['test'])

                metrics = test_results['metrics']
                self.logger.info(f"æµ‹è¯•MAE: {metrics['mae']:.3f}Â°C")
                self.logger.info(f"æµ‹è¯•RMSE: {metrics['rmse']:.3f}Â°C")
                self.logger.info(f"Tmax MAE: {metrics['Tmax_mae']:.3f}Â°C, RÂ²: {metrics['Tmax_r2']:.3f}")
                self.logger.info(f"Tmin MAE: {metrics['Tmin_mae']:.3f}Â°C, RÂ²: {metrics['Tmin_r2']:.3f}")

                # ä¿å­˜æµ‹è¯•ç»“æœ
                test_path = self.output_dir / "test_results.pkl"
                with open(test_path, 'wb') as f:
                    import pickle
                    pickle.dump(test_results, f)

                self.logger.info(f"æµ‹è¯•ç»“æœä¿å­˜åˆ°: {test_path}")

                # 6. å¯è§†åŒ–
                self.logger.info("æ­¥éª¤6: å¯è§†åŒ–...")
                self.visualize_results(test_results)

            self.logger.info("=" * 60)
            self.logger.info("âœ… æ°”è±¡é¢„æµ‹å¾®è°ƒå®Œæˆ!")
            self.logger.info("=" * 60)

            return {
                'model': self.model,
                'history': history,
                'test_results': test_results if 'test' in dataloaders else None
            }

        except Exception as e:
            self.logger.error(f"å¾®è°ƒå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            raise


def main():
    """ä¸»å‡½æ•°"""
    # åˆ›å»ºå¾®è°ƒå™¨
    finetuner = WeatherFinetuner()

    # è¿è¡Œå¾®è°ƒæµæ°´çº¿
    results = finetuner.run_pipeline()

    return results


if __name__ == "__main__":
    main()