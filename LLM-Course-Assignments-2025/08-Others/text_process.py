"""
æ°”å€™æ–‡æœ¬çŸ¥è¯†å¤„ç†æ¨¡å— - ä¿®å¤å­—ä½“é—®é¢˜ç‰ˆæœ¬
å¤„ç†JSONæ ¼å¼çš„æ°”å€™çŸ¥è¯†æ–‡æœ¬ï¼Œç”Ÿæˆè¯­ä¹‰åµŒå…¥å‘é‡ç”¨äºæ™ºèƒ½ä½“ç³»ç»Ÿ
"""

import json
import pickle
import numpy as np
import pandas as pd
from typing import Dict, List, Tuple, Any, Optional, Union
from pathlib import Path
import warnings
warnings.filterwarnings('ignore')
from sentence_transformers import SentenceTransformer
import torch
from tqdm import tqdm
import re
import hashlib
import sys

class ClimateKnowledgeProcessor:
    """æ°”å€™çŸ¥è¯†æ–‡æœ¬å¤„ç†å™¨"""

    def __init__(self,
                 json_path: str = "/home/Liyang/agent/data/climate_knowledge.json",
                 output_dir: str = "/home/Liyang/agent/processed_data/text_features",
                 embedding_model: str = "paraphrase-multilingual-MiniLM-L12-v2"):
        """
        åˆå§‹åŒ–æ–‡æœ¬å¤„ç†å™¨
        """
        self.json_path = Path(json_path)
        self.output_dir = Path(output_dir)
        self.embedding_model_name = embedding_model

        # åˆ›å»ºè¾“å‡ºç›®å½•
        self.output_dir.mkdir(parents=True, exist_ok=True)

        # åŠ è½½åµŒå…¥æ¨¡å‹
        print(f"ğŸ“š åŠ è½½æ–‡æœ¬åµŒå…¥æ¨¡å‹: {self.embedding_model_name}")
        try:
            self.model = SentenceTransformer(self.embedding_model_name)
            self.embedding_dim = self.model.get_sentence_embedding_dimension()
            print(f"âœ“ åµŒå…¥æ¨¡å‹åŠ è½½æˆåŠŸï¼Œç»´åº¦: {self.embedding_dim}")
        except Exception as e:
            print(f"âœ— æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            # ä½¿ç”¨å¤‡ç”¨æ¨¡å‹
            self.model = SentenceTransformer('all-MiniLM-L6-v2')
            self.embedding_dim = self.model.get_sentence_embedding_dimension()
            print(f"âœ“ ä½¿ç”¨å¤‡ç”¨æ¨¡å‹ï¼Œç»´åº¦: {self.embedding_dim}")

        # æ•°æ®å­˜å‚¨
        self.knowledge_items = []
        self.embeddings = None
        self.metadata = {
            "total_items": 0,
            "categories": set(),
            "data_fields": set(),
            "processed_date": None
        }

    def load_and_validate_data(self) -> List[Dict]:
        """
        åŠ è½½å¹¶éªŒè¯JSONæ•°æ®
        """
        print(f"\nğŸ“‚ åŠ è½½æ•°æ®æ–‡ä»¶: {self.json_path}")

        if not self.json_path.exists():
            print(f"âœ— æ–‡ä»¶ä¸å­˜åœ¨: {self.json_path}")
            print("è¯·åˆ›å»ºç¤ºä¾‹æ–‡ä»¶æˆ–æ£€æŸ¥è·¯å¾„")
            return []

        try:
            with open(self.json_path, 'r', encoding='utf-8') as f:
                data = json.load(f)

            # éªŒè¯æ•°æ®æ ¼å¼
            if isinstance(data, list):
                self.knowledge_items = data
            elif isinstance(data, dict):
                # å¦‚æœæ˜¯å­—å…¸ï¼Œå¯èƒ½æ˜¯åŒ…å«å¤šä¸ªæ¡ç›®çš„å¯¹è±¡
                if "knowledge" in data:
                    self.knowledge_items = data["knowledge"]
                elif "items" in data:
                    self.knowledge_items = data["items"]
                else:
                    # å°è¯•è½¬æ¢å•ä¸ªå¯¹è±¡ä¸ºåˆ—è¡¨
                    self.knowledge_items = [data]
            else:
                raise ValueError(f"âŒ æœªçŸ¥çš„JSONæ ¼å¼: {type(data)}")

            print(f"âœ“ æˆåŠŸåŠ è½½ {len(self.knowledge_items)} æ¡çŸ¥è¯†æ¡ç›®")

            # æ˜¾ç¤ºå‰å‡ æ¡æ•°æ®
            for i, item in enumerate(self.knowledge_items[:3]):
                print(f"\næ ·æœ¬æ¡ç›® {i+1}:")
                for key, value in item.items():
                    if isinstance(value, list):
                        print(f"  {key}: {', '.join(value[:5])}{'...' if len(value) > 5 else ''}")
                    else:
                        print(f"  {key}: {str(value)[:100]}{'...' if len(str(value)) > 100 else ''}")

        except json.JSONDecodeError as e:
            print(f"âŒ JSONè§£æé”™è¯¯: {e}")
            print(f"è¯·æ£€æŸ¥ {self.json_path} æ–‡ä»¶æ ¼å¼")
            return []
        except Exception as e:
            print(f"âŒ åŠ è½½æ•°æ®å¤±è´¥: {e}")
            return []

        # éªŒè¯æ¯ä¸ªæ¡ç›®çš„ç»“æ„
        valid_items = []
        for i, item in enumerate(self.knowledge_items):
            if self._validate_item(item, i):
                valid_items.append(item)

        self.knowledge_items = valid_items
        print(f"âœ“ æœ‰æ•ˆæ¡ç›®: {len(self.knowledge_items)} æ¡")

        # æå–å…ƒæ•°æ®
        self._extract_metadata()

        return self.knowledge_items

    def _validate_item(self, item: Dict, index: int) -> bool:
        """
        éªŒè¯å•ä¸ªçŸ¥è¯†æ¡ç›®çš„ç»“æ„
        """
        # å¿…éœ€å­—æ®µæ£€æŸ¥
        required_fields = ['category', 'title', 'scientific_basis']
        missing_fields = [field for field in required_fields if field not in item]

        if missing_fields:
            print(f"âš ï¸ æ¡ç›® {index} ç¼ºå°‘å¿…éœ€å­—æ®µ: {missing_fields}")
            return False

        # å­—æ®µç±»å‹æ£€æŸ¥
        if not isinstance(item['category'], str):
            print(f"âš ï¸ æ¡ç›® {index} çš„categoryå­—æ®µä¸æ˜¯å­—ç¬¦ä¸²ç±»å‹")
            return False

        if not isinstance(item['title'], str):
            print(f"âš ï¸ æ¡ç›® {index} çš„titleå­—æ®µä¸æ˜¯å­—ç¬¦ä¸²ç±»å‹")
            return False

        if not isinstance(item['scientific_basis'], str):
            print(f"âš ï¸ æ¡ç›® {index} çš„scientific_basiså­—æ®µä¸æ˜¯å­—ç¬¦ä¸²ç±»å‹")
            return False

        return True

    def _extract_metadata(self):
        """æå–æ•°æ®é›†çš„å…ƒæ•°æ®"""
        self.metadata["total_items"] = len(self.knowledge_items)
        self.metadata["processed_date"] = pd.Timestamp.now().isoformat()

        # æ”¶é›†æ‰€æœ‰ç±»åˆ«
        categories = set()
        data_fields = set()

        for item in self.knowledge_items:
            categories.add(item['category'])

            # æ”¶é›†ç›¸å…³æ•°æ®å­—æ®µ
            if 'related_data_fields' in item and isinstance(item['related_data_fields'], list):
                for field in item['related_data_fields']:
                    data_fields.add(field)

        self.metadata["categories"] = list(categories)
        self.metadata["data_fields"] = list(data_fields)

    def preprocess_text(self, item: Dict) -> List[str]:
        """
        é¢„å¤„ç†å•ä¸ªçŸ¥è¯†æ¡ç›®ï¼Œç”Ÿæˆå¤šä¸ªæ–‡æœ¬è¡¨ç¤º
        """
        texts = []

        # 1. å®Œæ•´è¡¨ç¤ºï¼ˆæ‰€æœ‰ä¿¡æ¯ï¼‰
        full_text = f"ç±»åˆ«ï¼š{item['category']} æ ‡é¢˜ï¼š{item['title']} ç§‘å­¦ä¾æ®ï¼š{item['scientific_basis']}"

        if 'warning_indicators' in item:
            full_text += f" é¢„è­¦æŒ‡æ ‡ï¼š{item['warning_indicators']}"

        if 'adaptive_actions' in item:
            full_text += f" åº”å¯¹æªæ–½ï¼š{item['adaptive_actions']}"

        texts.append(full_text.strip())

        # 2. ç§‘å­¦ä¾æ®+é¢„è­¦æŒ‡æ ‡ï¼ˆç”¨äºé£é™©è¯„ä¼°ï¼‰
        if 'warning_indicators' in item:
            risk_text = f"{item['scientific_basis']} {item['warning_indicators']}"
            texts.append(risk_text)

        # 3. ç®€æ´è¡¨ç¤ºï¼ˆç”¨äºå¿«é€Ÿæ£€ç´¢ï¼‰
        concise_text = f"{item['category']}ï¼š{item['title']} - {item['scientific_basis'][:100]}..."
        texts.append(concise_text)

        # 4. ä»…ç§‘å­¦ä¾æ®ï¼ˆç”¨äºåŒ¹é…æ°”è±¡æ•°æ®ï¼‰
        texts.append(item['scientific_basis'])

        return texts

    def extract_keywords(self, text: str) -> List[str]:
        """
        æå–æ–‡æœ¬ä¸­çš„å…³é”®è¯
        """
        # æå–ä¸­æ–‡å…³é”®è¯
        chinese_words = re.findall(r'[\u4e00-\u9fff]{2,}', text)

        # æå–è‹±æ–‡å˜é‡åï¼ˆå¦‚ Present_Tmaxï¼‰
        english_vars = re.findall(r'[A-Z][A-Za-z_]+', text)

        # æå–æ¸©åº¦é˜ˆå€¼ï¼ˆå¦‚ 32â„ƒ, 35â„ƒï¼‰
        temperature_thresholds = re.findall(r'\d+â„ƒ', text)

        # æå–ç™¾åˆ†æ¯”ï¼ˆå¦‚ 45%ï¼‰
        percentages = re.findall(r'\d+%', text)

        # åˆå¹¶æ‰€æœ‰å…³é”®è¯
        keywords = chinese_words + english_vars + temperature_thresholds + percentages

        # å»é‡å¹¶è¿‡æ»¤ç©ºå­—ç¬¦ä¸²
        keywords = [kw for kw in set(keywords) if kw.strip()]

        return keywords

    def generate_embeddings(self, batch_size: int = 32) -> np.ndarray:
        """
        ä¸ºæ‰€æœ‰çŸ¥è¯†æ¡ç›®ç”ŸæˆåµŒå…¥å‘é‡
        """
        print(f"\nğŸ”§ ç”Ÿæˆæ–‡æœ¬åµŒå…¥å‘é‡...")

        if not self.knowledge_items:
            print("âŒ æ²¡æœ‰å¯å¤„ç†çš„çŸ¥è¯†æ¡ç›®")
            return np.array([])

        # å‡†å¤‡æ–‡æœ¬æ•°æ®
        all_texts = []
        text_indices = []  # è®°å½•æ¯ä¸ªæ¡ç›®å¯¹åº”çš„æ–‡æœ¬ç´¢å¼•

        for idx, item in enumerate(tqdm(self.knowledge_items, desc="å‡†å¤‡æ–‡æœ¬")):
            texts = self.preprocess_text(item)
            all_texts.extend(texts)
            text_indices.append((idx, len(texts)))  # (æ¡ç›®ç´¢å¼•, æ–‡æœ¬æ•°é‡)

        print(f"âœ“ å…±ç”Ÿæˆ {len(all_texts)} ä¸ªæ–‡æœ¬è¡¨ç¤º")

        # ç”ŸæˆåµŒå…¥å‘é‡
        print(f"â³ æ­£åœ¨è®¡ç®—åµŒå…¥å‘é‡...")
        embeddings = self.model.encode(
            all_texts,
            batch_size=batch_size,
            show_progress_bar=True,
            convert_to_numpy=True,
            normalize_embeddings=True  # å½’ä¸€åŒ–ä»¥ä¾¿åç»­è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
        )

        print(f"âœ“ åµŒå…¥å‘é‡ç”Ÿæˆå®Œæˆï¼Œå½¢çŠ¶: {embeddings.shape}")

        # é‡æ–°ç»„ç»‡åµŒå…¥å‘é‡ï¼šä¸ºæ¯ä¸ªæ¡ç›®ç”Ÿæˆä¸€ä¸ªç»¼åˆåµŒå…¥
        self.embeddings = np.zeros((len(self.knowledge_items), self.embedding_dim))

        current_idx = 0
        for item_idx, (original_idx, num_texts) in enumerate(text_indices):
            # è·å–è¯¥æ¡ç›®çš„æ‰€æœ‰æ–‡æœ¬åµŒå…¥
            item_embeddings = embeddings[current_idx:current_idx + num_texts]

            # ä½¿ç”¨å¹³å‡æ± åŒ–ç”Ÿæˆç»¼åˆåµŒå…¥
            self.embeddings[item_idx] = np.mean(item_embeddings, axis=0)

            current_idx += num_texts

        # å½’ä¸€åŒ–æœ€ç»ˆåµŒå…¥
        norms = np.linalg.norm(self.embeddings, axis=1, keepdims=True)
        norms[norms == 0] = 1  # é¿å…é™¤ä»¥é›¶
        self.embeddings = self.embeddings / norms

        return self.embeddings

    def create_knowledge_base(self) -> Dict:
        """
        åˆ›å»ºç»“æ„åŒ–çš„çŸ¥è¯†åº“
        """
        print(f"\nğŸ—ï¸ åˆ›å»ºç»“æ„åŒ–çŸ¥è¯†åº“...")

        knowledge_base = {
            "metadata": self.metadata,
            "items": [],
            "embeddings": self.embeddings.tolist() if self.embeddings is not None else [],
            "index_mapping": {}  # ç±»åˆ«åˆ°æ¡ç›®ç´¢å¼•çš„æ˜ å°„
        }

        # æŒ‰ç±»åˆ«ç»„ç»‡æ¡ç›®
        category_to_indices = {}

        for idx, item in enumerate(self.knowledge_items):
            # å¤åˆ¶æ¡ç›®å¹¶æ·»åŠ å¤„ç†åçš„ä¿¡æ¯
            processed_item = item.copy()

            # æå–å…³é”®è¯
            full_text = self.preprocess_text(item)[0]
            keywords = self.extract_keywords(full_text)
            processed_item['keywords'] = keywords

            # ç”Ÿæˆæ¡ç›®ID
            item_hash = hashlib.md5(full_text.encode()).hexdigest()[:8]
            processed_item['item_id'] = f"KNOW_{item_hash}"

            # æ·»åŠ åˆ°çŸ¥è¯†åº“
            knowledge_base["items"].append(processed_item)

            # æ›´æ–°ç±»åˆ«æ˜ å°„
            category = item['category']
            if category not in category_to_indices:
                category_to_indices[category] = []
            category_to_indices[category].append(idx)

        # æ›´æ–°ç´¢å¼•æ˜ å°„
        knowledge_base["index_mapping"] = category_to_indices

        print(f"âœ“ çŸ¥è¯†åº“åˆ›å»ºå®Œæˆï¼ŒåŒ…å« {len(knowledge_base['items'])} ä¸ªæ¡ç›®")

        # æ˜¾ç¤ºç±»åˆ«åˆ†å¸ƒ
        print(f"âœ“ ç±»åˆ«åˆ†å¸ƒ:")
        for cat, indices in category_to_indices.items():
            print(f"  {cat}: {len(indices)} æ¡")

        return knowledge_base

    def save_results(self, knowledge_base: Dict):
        """
        ä¿å­˜å¤„ç†ç»“æœ
        """
        print(f"\nğŸ’¾ ä¿å­˜å¤„ç†ç»“æœåˆ° {self.output_dir}")

        # 1. ä¿å­˜å®Œæ•´çŸ¥è¯†åº“ä¸ºJSON
        json_path = self.output_dir / "knowledge_base.json"
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(knowledge_base, f, ensure_ascii=False, indent=2)
        print(f"âœ“ çŸ¥è¯†åº“ä¿å­˜åˆ°: {json_path}")

        # 2. ä¿å­˜åµŒå…¥å‘é‡ä¸ºnumpyæ–‡ä»¶
        if self.embeddings is not None:
            np_path = self.output_dir / "embeddings.npy"
            np.save(np_path, self.embeddings)
            print(f"âœ“ åµŒå…¥å‘é‡ä¿å­˜åˆ°: {np_path}")

        # 3. ä¿å­˜å…ƒæ•°æ®
        metadata_path = self.output_dir / "metadata.json"
        with open(metadata_path, 'w', encoding='utf-8') as f:
            json.dump(self.metadata, f, ensure_ascii=False, indent=2)
        print(f"âœ“ å…ƒæ•°æ®ä¿å­˜åˆ°: {metadata_path}")

        # 4. ä¿å­˜ä¸ºCSVæ ¼å¼ï¼ˆä¾¿äºæŸ¥çœ‹ï¼‰
        csv_data = []
        for item in knowledge_base["items"]:
            csv_row = {
                "item_id": item.get("item_id", ""),
                "category": item.get("category", ""),
                "title": item.get("title", ""),
                "scientific_basis": item.get("scientific_basis", ""),
                "keywords": ", ".join(item.get("keywords", [])),
                "related_data_fields": ", ".join(item.get("related_data_fields", [])),
                "warning_indicators": item.get("warning_indicators", ""),
                "adaptive_actions": item.get("adaptive_actions", "")
            }
            csv_data.append(csv_row)

        csv_path = self.output_dir / "knowledge_base.csv"
        pd.DataFrame(csv_data).to_csv(csv_path, index=False, encoding='utf-8-sig')
        print(f"âœ“ CSVæ ¼å¼ä¿å­˜åˆ°: {csv_path}")

        # 5. ä¿å­˜ä¸ºpickleæ ¼å¼ï¼ˆç”¨äºPythonå¿«é€ŸåŠ è½½ï¼‰
        pkl_path = self.output_dir / "knowledge_base.pkl"
        with open(pkl_path, 'wb') as f:
            pickle.dump(knowledge_base, f)
        print(f"âœ“ Pickleæ ¼å¼ä¿å­˜åˆ°: {pkl_path}")

    def analyze_knowledge_base(self, knowledge_base: Dict):
        """
        åˆ†æçŸ¥è¯†åº“å¹¶ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š
        """
        print(f"\nğŸ“Š çŸ¥è¯†åº“åˆ†ææŠ¥å‘Š")
        print("=" * 50)

        items = knowledge_base["items"]

        # 1. åŸºæœ¬ç»Ÿè®¡
        print(f"çŸ¥è¯†æ¡ç›®æ€»æ•°: {len(items)}")
        print(f"ç±»åˆ«æ•°é‡: {len(knowledge_base['index_mapping'])}")

        # 2. ç±»åˆ«åˆ†å¸ƒ
        print("\nğŸ“ˆ ç±»åˆ«åˆ†å¸ƒ:")
        for category, indices in knowledge_base["index_mapping"].items():
            percentage = len(indices) / len(items) * 100
            print(f"  {category}: {len(indices)} æ¡ ({percentage:.1f}%)")

        # 3. å…³é”®è¯ç»Ÿè®¡
        all_keywords = []
        for item in items:
            all_keywords.extend(item.get("keywords", []))

        from collections import Counter
        keyword_counts = Counter(all_keywords)

        print(f"\nğŸ”‘ é«˜é¢‘å…³é”®è¯ (Top 15):")
        for keyword, count in keyword_counts.most_common(15):
            print(f"  {keyword}: {count} æ¬¡")

        # 4. æ•°æ®å­—æ®µç»Ÿè®¡
        all_data_fields = []
        for item in items:
            all_data_fields.extend(item.get("related_data_fields", []))

        data_field_counts = Counter(all_data_fields)

        print(f"\nğŸ“‹ ç›¸å…³æ•°æ®å­—æ®µç»Ÿè®¡:")
        for field, count in data_field_counts.most_common():
            print(f"  {field}: {count} æ¬¡")

        # 5. æ–‡æœ¬é•¿åº¦åˆ†æ
        text_lengths = []
        for item in items:
            full_text = self.preprocess_text(item)[0]
            text_lengths.append(len(full_text))

        print(f"\nğŸ“ æ–‡æœ¬é•¿åº¦ç»Ÿè®¡:")
        print(f"  å¹³å‡é•¿åº¦: {np.mean(text_lengths):.0f} å­—ç¬¦")
        print(f"  æœ€å°é•¿åº¦: {np.min(text_lengths)} å­—ç¬¦")
        print(f"  æœ€å¤§é•¿åº¦: {np.max(text_lengths)} å­—ç¬¦")
        print(f"  æ ‡å‡†å·®: {np.std(text_lengths):.0f} å­—ç¬¦")

    def visualize_knowledge_base_simple(self, knowledge_base: Dict):
        """
        ç®€åŒ–ç‰ˆå¯è§†åŒ–ï¼ˆä¸éœ€è¦ä¸­æ–‡å­—ä½“ï¼‰
        """
        try:
            import matplotlib.pyplot as plt
            import seaborn as sns

            print(f"\nğŸ¨ ç”Ÿæˆç®€åŒ–å¯è§†åŒ–å›¾è¡¨...")

            # åˆ›å»ºå¯è§†åŒ–ç›®å½•
            viz_dir = self.output_dir / "visualizations"
            viz_dir.mkdir(exist_ok=True)

            # 1. ç±»åˆ«åˆ†å¸ƒæŸ±çŠ¶å›¾ï¼ˆä½¿ç”¨è‹±æ–‡æ ‡ç­¾é¿å…å­—ä½“é—®é¢˜ï¼‰
            categories = list(knowledge_base["index_mapping"].keys())
            counts = [len(indices) for indices in knowledge_base["index_mapping"].values()]

            # ç”Ÿæˆç®€çŸ­çš„ç±»åˆ«æ ‡ç­¾
            short_categories = []
            for cat in categories:
                # å–å‰å‡ ä¸ªå­—ç¬¦æˆ–ä½¿ç”¨ç¼©å†™
                if len(cat) > 10:
                    short_cat = cat[:8] + "..."
                else:
                    short_cat = cat
                short_categories.append(short_cat)

            plt.figure(figsize=(12, 6))
            bars = plt.bar(range(len(categories)), counts, alpha=0.7)
            plt.xticks(range(len(categories)), short_categories, rotation=45, ha='right')
            plt.title('Knowledge Category Distribution')
            plt.xlabel('Category')
            plt.ylabel('Count')

            # åœ¨æŸ±å­ä¸Šæ·»åŠ æ•°é‡æ ‡ç­¾
            for bar, count in zip(bars, counts):
                height = bar.get_height()
                plt.text(bar.get_x() + bar.get_width()/2., height,
                        f'{count}', ha='center', va='bottom')

            plt.tight_layout()
            plt.savefig(viz_dir / "category_distribution_en.png", dpi=300, bbox_inches='tight')
            plt.close()
            print(f"âœ“ ç±»åˆ«åˆ†å¸ƒå›¾ä¿å­˜åˆ°: {viz_dir}/category_distribution_en.png")

            # 2. å…³é”®è¯é¢‘ç‡æŸ±çŠ¶å›¾
            all_keywords = []
            for item in knowledge_base["items"]:
                all_keywords.extend(item.get("keywords", []))

            from collections import Counter
            keyword_counts = Counter(all_keywords)

            # å–å‰20ä¸ªå…³é”®è¯
            top_keywords = keyword_counts.most_common(15)
            keywords, counts = zip(*top_keywords) if top_keywords else ([], [])

            plt.figure(figsize=(12, 6))
            bars = plt.bar(range(len(keywords)), counts, alpha=0.7)
            plt.xticks(range(len(keywords)), keywords, rotation=45, ha='right')
            plt.title('Top Keywords Frequency')
            plt.xlabel('Keyword')
            plt.ylabel('Frequency')

            plt.tight_layout()
            plt.savefig(viz_dir / "keyword_frequency_en.png", dpi=300, bbox_inches='tight')
            plt.close()
            print(f"âœ“ å…³é”®è¯é¢‘ç‡å›¾ä¿å­˜åˆ°: {viz_dir}/keyword_frequency_en.png")

        except ImportError as e:
            print(f"âš ï¸ å¯è§†åŒ–ä¾èµ–åº“æœªå®‰è£…: {e}")
            print("è¯·è¿è¡Œ: pip install matplotlib seaborn")

    def visualize_knowledge_base_advanced(self, knowledge_base: Dict):
        """
        é«˜çº§å¯è§†åŒ–ï¼ˆå°è¯•ä½¿ç”¨ä¸­æ–‡å­—ä½“ï¼‰
        """
        try:
            import matplotlib
            import matplotlib.pyplot as plt
            import seaborn as sns
            from sklearn.manifold import TSNE

            print(f"\nğŸ¨ ç”Ÿæˆé«˜çº§å¯è§†åŒ–å›¾è¡¨...")

            # åˆ›å»ºå¯è§†åŒ–ç›®å½•
            viz_dir = self.output_dir / "visualizations"
            viz_dir.mkdir(exist_ok=True)

            # å°è¯•è®¾ç½®ä¸­æ–‡å­—ä½“ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            def try_set_chinese_font():
                """å°è¯•è®¾ç½®ä¸­æ–‡å­—ä½“"""
                # å¸¸è§çš„ä¸­æ–‡å­—ä½“è·¯å¾„
                font_paths = [
                    # Windows
                    "C:/Windows/Fonts/simhei.ttf",  # é»‘ä½“
                    "C:/Windows/Fonts/simsun.ttc",  # å®‹ä½“
                    # Mac
                    "/System/Library/Fonts/PingFang.ttc",
                    "/System/Library/Fonts/STHeiti Medium.ttc",
                    # Linux
                    "/usr/share/fonts/truetype/wqy/wqy-microhei.ttc",
                    "/usr/share/fonts/truetype/arphic/uming.ttc",
                ]

                for font_path in font_paths:
                    if Path(font_path).exists():
                        try:
                            matplotlib.font_manager.fontManager.addfont(font_path)
                            font_name = matplotlib.font_manager.FontProperties(fname=font_path).get_name()
                            matplotlib.rcParams['font.sans-serif'] = [font_name]
                            matplotlib.rcParams['axes.unicode_minus'] = False
                            print(f"âœ“ ä½¿ç”¨ä¸­æ–‡å­—ä½“: {font_name}")
                            return True
                        except:
                            continue

                print("âš ï¸ æœªæ‰¾åˆ°å¯ç”¨çš„ä¸­æ–‡å­—ä½“ï¼Œä½¿ç”¨é»˜è®¤å­—ä½“")
                return False

            # å°è¯•è®¾ç½®ä¸­æ–‡å­—ä½“
            has_chinese_font = try_set_chinese_font()

            # 1. ç±»åˆ«åˆ†å¸ƒé¥¼å›¾
            categories = list(knowledge_base["index_mapping"].keys())
            counts = [len(indices) for indices in knowledge_base["index_mapping"].values()]

            plt.figure(figsize=(10, 8))
            plt.pie(counts, labels=categories, autopct='%1.1f%%', startangle=90)
            plt.title('çŸ¥è¯†æ¡ç›®ç±»åˆ«åˆ†å¸ƒ')
            plt.tight_layout()

            if has_chinese_font:
                plt.savefig(viz_dir / "category_distribution_cn.png", dpi=300, bbox_inches='tight')
                print(f"âœ“ ä¸­æ–‡ç±»åˆ«åˆ†å¸ƒå›¾ä¿å­˜åˆ°: {viz_dir}/category_distribution_cn.png")
            else:
                # å¦‚æœæ— æ³•æ˜¾ç¤ºä¸­æ–‡ï¼Œä½¿ç”¨è‹±æ–‡æ ‡ç­¾
                plt.title('Knowledge Category Distribution')
                plt.savefig(viz_dir / "category_distribution.png", dpi=300, bbox_inches='tight')
                print(f"âœ“ ç±»åˆ«åˆ†å¸ƒå›¾ä¿å­˜åˆ°: {viz_dir}/category_distribution.png")

            plt.close()

            # 2. åµŒå…¥å‘é‡å¯è§†åŒ–ï¼ˆt-SNEï¼‰
            if self.embeddings is not None and len(self.embeddings) > 10:
                # é™ç»´
                n_samples = min(100, len(self.embeddings))
                indices = np.random.choice(len(self.embeddings), n_samples, replace=False)
                embeddings_sample = self.embeddings[indices]

                # t-SNE
                tsne = TSNE(n_components=2, random_state=42, perplexity=min(30, n_samples-1))
                embeddings_2d = tsne.fit_transform(embeddings_sample)

                # è·å–ç±»åˆ«æ ‡ç­¾
                labels = []
                for idx in indices:
                    # æ‰¾åˆ°å¯¹åº”çš„æ¡ç›®
                    for category, cat_indices in knowledge_base["index_mapping"].items():
                        if idx in cat_indices:
                            labels.append(category)
                            break
                    else:
                        labels.append("Unknown")

                # ç»˜åˆ¶
                plt.figure(figsize=(12, 10))
                scatter = plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1],
                                    c=[hash(label) % 20 for label in labels],
                                    cmap='tab20', alpha=0.7, s=100)

                # æ·»åŠ å›¾ä¾‹
                unique_labels = list(set(labels))
                handles = [plt.Line2D([0], [0], marker='o', color='w',
                                     markerfacecolor=plt.cm.tab20(hash(label) % 20 / 20),
                                     markersize=10) for label in unique_labels]
                plt.legend(handles, unique_labels, title="ç±»åˆ«", bbox_to_anchor=(1.05, 1), loc='upper left')

                plt.title('çŸ¥è¯†æ¡ç›®åµŒå…¥å‘é‡å¯è§†åŒ– (t-SNE)')
                plt.xlabel('t-SNE ç»´åº¦ 1')
                plt.ylabel('t-SNE ç»´åº¦ 2')
                plt.tight_layout()
                plt.savefig(viz_dir / "embeddings_tsne.png", dpi=300, bbox_inches='tight')
                plt.close()
                print(f"âœ“ åµŒå…¥å‘é‡å¯è§†åŒ–ä¿å­˜åˆ°: {viz_dir}/embeddings_tsne.png")

        except ImportError as e:
            print(f"âš ï¸ é«˜çº§å¯è§†åŒ–ä¾èµ–åº“æœªå®‰è£…: {e}")
            print("è¯·è¿è¡Œ: pip install matplotlib seaborn scikit-learn")

    def run_pipeline(self, enable_visualization: bool = True):
        """
        è¿è¡Œå®Œæ•´çš„æ•°æ®å¤„ç†æµæ°´çº¿
        """
        print("=" * 60)
        print("ğŸŒ æ°”å€™æ–‡æœ¬çŸ¥è¯†å¤„ç†æµæ°´çº¿")
        print("=" * 60)

        # æ­¥éª¤1: åŠ è½½æ•°æ®
        data = self.load_and_validate_data()

        if not data:
            print("âŒ æ²¡æœ‰æ•°æ®å¯å¤„ç†ï¼Œç¨‹åºé€€å‡º")
            return None

        # æ­¥éª¤2: ç”ŸæˆåµŒå…¥å‘é‡
        self.generate_embeddings()

        # æ­¥éª¤3: åˆ›å»ºçŸ¥è¯†åº“
        knowledge_base = self.create_knowledge_base()

        # æ­¥éª¤4: åˆ†æçŸ¥è¯†åº“
        self.analyze_knowledge_base(knowledge_base)

        # æ­¥éª¤5: å¯è§†åŒ–ï¼ˆå¯é€‰ï¼‰
        if enable_visualization:
            try:
                # å°è¯•é«˜çº§å¯è§†åŒ–
                self.visualize_knowledge_base_advanced(knowledge_base)
            except Exception as e:
                print(f"âš ï¸ é«˜çº§å¯è§†åŒ–å¤±è´¥: {e}")
                # å›é€€åˆ°ç®€åŒ–ç‰ˆ
                self.visualize_knowledge_base_simple(knowledge_base)

        # æ­¥éª¤6: ä¿å­˜ç»“æœ
        self.save_results(knowledge_base)

        print("\n" + "=" * 60)
        print("âœ… æ°”å€™æ–‡æœ¬æ•°æ®å¤„ç†å®Œæˆ!")
        print("=" * 60)

        # æ‰“å°è¾“å‡ºæ–‡ä»¶æ‘˜è¦
        print(f"\nğŸ“ è¾“å‡ºæ–‡ä»¶:")
        for file_path in self.output_dir.iterdir():
            if file_path.is_file():
                file_size = file_path.stat().st_size / 1024  # KB
                print(f"  {file_path.name} ({file_size:.1f} KB)")

        return knowledge_base

# ============================ å‘½ä»¤è¡Œæ¥å£ ============================
def process_climate_knowledge():
    """å¤„ç†æ°”å€™çŸ¥è¯†æ–‡æœ¬çš„ä¸»å‡½æ•°"""

    # åˆ›å»ºå¤„ç†å™¨å®ä¾‹
    processor = ClimateKnowledgeProcessor(
        json_path="/home/Liyang/agent/knowledge_base/knowledge_base.json",
        output_dir="/home/Liyang/agent/knowledge_base",
        embedding_model="paraphrase-multilingual-MiniLM-L12-v2"
    )

    # è¿è¡Œå®Œæ•´æµæ°´çº¿
    knowledge_base = processor.run_pipeline(enable_visualization=True)

    if knowledge_base:
        print(f"\nğŸ”— æ™ºèƒ½ä½“ç³»ç»Ÿé›†æˆç¤ºä¾‹:")
        print("""
    åœ¨æ‚¨çš„æ™ºèƒ½ä½“ç³»ç»Ÿä¸­ä½¿ç”¨ï¼š
    
    1. åŠ è½½çŸ¥è¯†åº“ï¼š
        import pickle
        with open('/home/Liyang/agent/processed_data/text_features/knowledge_base.pkl', 'rb') as f:
            knowledge_base = pickle.load(f)
    
    2. æ£€ç´¢ç›¸å…³çŸ¥è¯†ï¼š
        import numpy as np
        from sentence_transformers import SentenceTransformer
        
        # åŠ è½½æ¨¡å‹
        model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
        
        # å®šä¹‰æ£€ç´¢å‡½æ•°
        def retrieve_relevant_knowledge(query_text, knowledge_base, top_k=3):
            # ç”ŸæˆæŸ¥è¯¢åµŒå…¥
            query_embedding = model.encode([query_text])[0]
            
            # è®¡ç®—ç›¸ä¼¼åº¦
            similarities = np.dot(knowledge_base['embeddings'], query_embedding)
            
            # è¿”å›æœ€ç›¸å…³çš„çŸ¥è¯†
            top_indices = np.argsort(similarities)[-top_k:][::-1]
            results = []
            for idx in top_indices:
                item = knowledge_base['items'][idx]
                results.append({
                    'item_id': item['item_id'],
                    'category': item['category'],
                    'title': item['title'],
                    'scientific_basis': item['scientific_basis'],
                    'similarity': similarities[idx]
                })
            return results
    
    3. ç¤ºä¾‹æŸ¥è¯¢ï¼š
        # å‡è®¾æœ‰æ°”è±¡æ•°æ®
        weather_data = {
            'Present_Tmax': 35.5,
            'LDAPS_RHmin': 40.0
        }
        
        # åˆ›å»ºæŸ¥è¯¢æ–‡æœ¬
        query = f"æ¸©åº¦{weather_data['Present_Tmax']}â„ƒ æ¹¿åº¦{weather_data['LDAPS_RHmin']}%"
        
        # æ£€ç´¢ç›¸å…³çŸ¥è¯†
        relevant_knowledge = retrieve_relevant_knowledge(query, knowledge_base)
        
        for i, knowledge in enumerate(relevant_knowledge):
            print(f"{i+1}. [{knowledge['category']}] {knowledge['title']}")
            print(f"   ç›¸ä¼¼åº¦: {knowledge['similarity']:.3f}")
            print(f"   ç§‘å­¦ä¾æ®: {knowledge['scientific_basis'][:100]}...")
            print()
        """)

    return knowledge_base

# ============================ å®‰è£…ä¾èµ–è„šæœ¬ ============================
def install_dependencies():
    """å®‰è£…å¿…è¦çš„ä¾èµ–åŒ…"""
    print("å®‰è£…æ–‡æœ¬å¤„ç†æ‰€éœ€ä¾èµ–...")

    dependencies = [
        "sentence-transformers",
        "numpy",
        "pandas",
        "tqdm",
        "scikit-learn",
        "matplotlib",
        "seaborn",
    ]

    import subprocess
    import sys

    for package in dependencies:
        print(f"æ­£åœ¨å®‰è£… {package}...")
        try:
            subprocess.check_call([sys.executable, "-m", "pip", "install", package])
            print(f"âœ“ {package} å®‰è£…æˆåŠŸ")
        except subprocess.CalledProcessError:
            print(f"âœ— {package} å®‰è£…å¤±è´¥ï¼Œè¯·æ‰‹åŠ¨å®‰è£…: pip install {package}")

    print("\næ‰€æœ‰ä¾èµ–å®‰è£…å®Œæˆï¼")

# ============================ åˆ›å»ºç¤ºä¾‹æ•°æ®è„šæœ¬ ============================
def create_sample_data():
    """åˆ›å»ºç¤ºä¾‹JSONæ•°æ®æ–‡ä»¶"""
    sample_data = [
        {
            "category": "æˆ·å¤–èµ›äº‹çƒ­æµªå®‰å…¨",
            "title": "æˆ·å¤–èµ›äº‹ / æ´»åŠ¨çš„çƒ­æµªå®‰å…¨é˜ˆå€¼",
            "scientific_basis": "Present_Tmaxâ‰¥32â„ƒ+LDAPS_RHminâ‰¤45% æ—¶ï¼Œé•¿æ—¶é—´æˆ·å¤–å‰§çƒˆè¿åŠ¨æ˜“å¼•å‘ç¾¤ä½“æ€§çƒ­å°„ç—…ï¼Œéœ€è®¾å®šèµ›äº‹ä¸¾åŠçš„å®‰å…¨çº¢çº¿",
            "related_data_fields": ["Present_Tmax", "LDAPS_RHmin", "lat", "lon"],
            "warning_indicators": "ä½é£é™©ï¼šPresent_Tmax<32â„ƒï¼›ä¸­é£é™©ï¼š32â„ƒâ‰¤Present_Tmax<34â„ƒï¼›é«˜é£é™©ï¼šPresent_Tmaxâ‰¥34â„ƒæˆ– LDAPS_RHminâ‰¤45%",
            "adaptive_actions": "ä½é£é™©æ—¶æ­£å¸¸ä¸¾åŠï¼›ä¸­é£é™©æ—¶ç¼©çŸ­èµ›äº‹æ—¶é•¿ã€å¢åŠ è¡¥æ°´ç‚¹ï¼›é«˜é£é™©æ—¶å»¶æœŸæˆ–å–æ¶ˆèµ›äº‹"
        },
        {
            "category": "å†œä¸šçƒ­æµªé¢„è­¦",
            "title": "å†œä½œç‰©é«˜æ¸©çƒ­å®³é¢„è­¦",
            "scientific_basis": "è¿ç»­3å¤©Present_Tmaxâ‰¥35â„ƒæˆ–æ—¥æœ€é«˜æ°”æ¸©â‰¥38â„ƒæ—¶ï¼Œæ°´ç¨»ã€ç‰ç±³ç­‰ä½œç‰©æ˜“å—é«˜æ¸©çƒ­å®³ï¼Œå½±å“æˆç²‰çŒæµ†",
            "related_data_fields": ["Present_Tmax", "Present_Tmin", "LDAPS_RHmin"],
            "warning_indicators": "è½»åº¦çƒ­å®³ï¼š35â„ƒâ‰¤Present_Tmax<37â„ƒæŒç»­3å¤©ï¼›ä¸­åº¦çƒ­å®³ï¼šPresent_Tmaxâ‰¥37â„ƒæŒç»­2å¤©ï¼›é‡åº¦çƒ­å®³ï¼šPresent_Tmaxâ‰¥40â„ƒ",
            "adaptive_actions": "è½»åº¦æ—¶å¢åŠ çŒæº‰ï¼›ä¸­åº¦æ—¶å–·æ–½å¶é¢è‚¥ï¼›é‡åº¦æ—¶è€ƒè™‘æå‰æ”¶å‰²"
        },
        {
            "category": "åŸå¸‚çƒ­å²›æ•ˆåº”",
            "title": "åŸå¸‚çƒ­æµªå¥åº·é£é™©",
            "scientific_basis": "åŸå¸‚åœ°åŒºç”±äºçƒ­å²›æ•ˆåº”ï¼Œå¤œé—´æ¸©åº¦æ¯”éƒŠåŒºé«˜2-5â„ƒï¼Œå¢åŠ å±…æ°‘çƒ­ç›¸å…³ç–¾ç—…é£é™©ï¼Œç‰¹åˆ«æ˜¯è€å¹´äººå’Œå„¿ç«¥",
            "related_data_fields": ["Present_Tmin", "LDAPS_Tmax_lapse", "lat", "lon", "DEM"],
            "warning_indicators": "å…³æ³¨å¤œé—´æ¸©åº¦Present_Tminâ‰¥28â„ƒæˆ–æ—¥æ¸©å·®<5â„ƒçš„æƒ…å†µ",
            "adaptive_actions": "å¼€æ”¾é¿æš‘åœºæ‰€ï¼Œå»¶é•¿å…¬å…±åœºæ‰€å¼€æ”¾æ—¶é—´ï¼Œå‘å¸ƒå¥åº·æé†’"
        },
        {
            "category": "èƒ½æºç”µåŠ›éœ€æ±‚",
            "title": "é«˜æ¸©å¤©æ°”ç”µåŠ›è´Ÿè·é¢„æµ‹",
            "scientific_basis": "æ°”æ¸©æ¯å‡é«˜1â„ƒï¼ŒåŸå¸‚ç”µåŠ›è´Ÿè·å¢åŠ 3-5%ï¼Œå½“Present_Tmaxâ‰¥35â„ƒæ—¶ï¼Œç©ºè°ƒè´Ÿè·å¯èƒ½å æ€»è´Ÿè·çš„40%ä»¥ä¸Š",
            "related_data_fields": ["Present_Tmax", "LDAPS_Tmax_lapse", "Solar radiation"],
            "warning_indicators": "é»„è‰²é¢„è­¦ï¼š35â„ƒâ‰¤Present_Tmax<37â„ƒï¼›æ©™è‰²é¢„è­¦ï¼š37â„ƒâ‰¤Present_Tmax<39â„ƒï¼›çº¢è‰²é¢„è­¦ï¼šPresent_Tmaxâ‰¥39â„ƒ",
            "adaptive_actions": "é»„è‰²é¢„è­¦æ—¶å¯åŠ¨æœ‰åºç”¨ç”µé¢„æ¡ˆï¼›æ©™è‰²é¢„è­¦æ—¶é™åˆ¶å·¥ä¸šç”¨ç”µï¼›çº¢è‰²é¢„è­¦æ—¶é‡‡å–è½®æ¢åœç”µæªæ–½"
        },
        {
            "category": "äº¤é€šå‡ºè¡Œå®‰å…¨",
            "title": "é«˜æ¸©å¤©æ°”é“è·¯å®‰å…¨",
            "scientific_basis": "è·¯é¢æ¸©åº¦å¯è¾¾æ°”æ¸©çš„1.5-2å€ï¼Œå½“Present_Tmaxâ‰¥35â„ƒæ—¶ï¼Œæ²¥é’è·¯é¢æ¸©åº¦å¯èƒ½è¶…è¿‡60â„ƒï¼Œå¢åŠ çˆ†èƒé£é™©",
            "related_data_fields": ["Present_Tmax", "Solar radiation", "LDAPS_WS"],
            "warning_indicators": "é“è·¯é«˜æ¸©é¢„è­¦ï¼šè·¯é¢æ¸©åº¦â‰¥55â„ƒï¼›äº¤é€šç®¡åˆ¶å»ºè®®ï¼šè·¯é¢æ¸©åº¦â‰¥65â„ƒ",
            "adaptive_actions": "é«˜æ¸©æ—¶æ®µå‡å°‘è¿è¾“ä»»åŠ¡ï¼Œå¢åŠ é“è·¯æ´’æ°´é™æ¸©ï¼Œæé†’è½¦è¾†æ£€æŸ¥èƒå‹"
        }
    ]

    output_path = Path("/home/Liyang/agent/knowledge_base")
    output_path.parent.mkdir(parents=True, exist_ok=True)

    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(sample_data, f, ensure_ascii=False, indent=2)

    print(f"âœ“ ç¤ºä¾‹æ•°æ®å·²åˆ›å»º: {output_path}")
    print(f"âœ“ åŒ…å« {len(sample_data)} æ¡çŸ¥è¯†æ¡ç›®")

    return sample_data

# ============================ ä¸»æ‰§è¡Œå‡½æ•° ============================
if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description='æ°”å€™æ–‡æœ¬çŸ¥è¯†å¤„ç†å·¥å…·')
    parser.add_argument('--install', action='store_true', help='å®‰è£…ä¾èµ–åŒ…')
    parser.add_argument('--create-sample', action='store_true', help='åˆ›å»ºç¤ºä¾‹æ•°æ®')
    parser.add_argument('--process', action='store_true', help='å¤„ç†æ–‡æœ¬æ•°æ®')
    parser.add_argument('--no-visualization', action='store_true', help='ä¸ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨')

    args = parser.parse_args()

    if args.install:
        install_dependencies()

    if args.create_sample:
        create_sample_data()

    if args.process or (not args.install and not args.create_sample):
        # é»˜è®¤è¿è¡Œæ•°æ®å¤„ç†
        process_climate_knowledge()

    if not any([args.install, args.create_sample, args.process]):
        # å¦‚æœæ²¡æœ‰æŒ‡å®šä»»ä½•å‚æ•°ï¼Œæ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯
        print("æ°”å€™æ–‡æœ¬çŸ¥è¯†å¤„ç†å·¥å…·")
        print("=" * 50)
        print("ä½¿ç”¨æ–¹æ³•:")
        print("  python climate_text_processor.py --install      # å®‰è£…ä¾èµ–")
        print("  python climate_text_processor.py --create-sample # åˆ›å»ºç¤ºä¾‹æ•°æ®")
        print("  python climate_text_processor.py --process      # å¤„ç†æ–‡æœ¬æ•°æ®")
        print("\næˆ–ç›´æ¥è¿è¡Œ: python climate_text_processor.py")